{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, re, codecs, json, time\n",
    "import pickle, collections, random, math, numbers, scipy.sparse as sp, itertools, shutil\n",
    "\n",
    "def reload(mName):\n",
    "    import importlib\n",
    "    if mName in sys.modules:\n",
    "        del sys.modules[mName]\n",
    "    return importlib.import_module(mName)\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "utils = reload('utils.utils')\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Movielens smallest data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"./data/ml-latest-small/ratings.csv\")\n",
    "movies = pd.read_csv(\"./data/ml-latest-small/movies.csv\")\n",
    "tags = pd.read_csv(\"./data/ml-latest-small/tags.csv\")\n",
    "\n",
    "uidEnc, midEnc = LabelEncoder(), LabelEncoder()\n",
    "uidEnc.fit(ratings.userId)\n",
    "midEnc.fit(movies.movieId)\n",
    "# encode user id and movie id to real value\n",
    "midEnc.fit(movies.movieId)\n",
    "uidEnc.fit(ratings.userId)\n",
    "\n",
    "ratings[\"userId\"] = uidEnc.transform(ratings.userId)\n",
    "ratings[\"movieId\"] = midEnc.transform(ratings.movieId)\n",
    "\n",
    "movies[\"movieId\"] = midEnc.transform(movies.movieId)\n",
    "\n",
    "tags[\"userId\"] = uidEnc.transform(tags.userId)\n",
    "tags[\"movieId\"] = midEnc.transform(tags.movieId)\n",
    "\n",
    "midMap = pd.Series(dict(zip(movies.movieId, movies.title)))\n",
    "\n",
    "nUsers, nMovies = len(uidEnc.classes_), len(midEnc.classes_)\n",
    "print(ratings.shape)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# movies profile\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tags profile\n",
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 已經產生出檔案的不用跑這一段\n",
    "# tr, te = utils.split_ratings(ratings, testRatio=0.3)\n",
    "# tr.to_csv(\"./data/ml-latest-small/movielens.tr.csv\", index=False)\n",
    "# te.to_csv(\"./data/ml-latest-small/movielens.te.csv\", index=False)\n",
    "# utils.dumpPickle(\"./data/ml-latest-small/state.h\", \n",
    "#         {\"uidEnc\": uidEnc, \"midEnc\": midEnc, \"midMap\": midMap, \"nUsers\": nUsers, \"nMovies\":nMovies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr = pd.read_csv(\"./data/ml-latest-small/movielens.tr.csv\")\n",
    "te = pd.read_csv(\"./data/ml-latest-small/movielens.te.csv\")\n",
    "\n",
    "# state = utils.loadPickle(\"./data/ml-latest-small/state.h\")\n",
    "# uidEnc, midEnc, midMap, nUsers, nMovies = \\\n",
    "#     (state[\"uidEnc\"], state[\"midEnc\"], state[\"midMap\"], state[\"nUsers\"], state[\"nMovies\"])\n",
    "\n",
    "# train data rating matrix\n",
    "trRatingMat = np.zeros((nUsers, nMovies))\n",
    "# test data rating matrix\n",
    "teRatingMat = np.zeros((nUsers, nMovies))\n",
    "for idx, r in tr.iterrows():\n",
    "    trRatingMat[int(r.userId), int(r.movieId)] = r.rating\n",
    "for idx, r in te.iterrows():\n",
    "    teRatingMat[int(r.userId), int(r.movieId)] = r.rating\n",
    "\n",
    "print(\"train interaction matrix shape: \", trRatingMat.shape, \"test interaction matrix shape: \", teRatingMat.shape)\n",
    "print(\"train.shape: \", tr.shape, \"test.shape: \", te.shape)\n",
    "print()\n",
    "print(tr.head())\n",
    "print()\n",
    "print(te.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Function\n",
    "1. batch產生資料, 每筆資料有三個欄位 user id, movie id, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataFn(data, n_batch=256, shuffle=False):\n",
    "    data = data.copy()\n",
    "    # 讓rating分數只有1 or 0(喜歡 or 不喜歡)\n",
    "    # data[\"rating\"] = data.rating.map(lambda e: e >= 4).astype(int)\n",
    "    def _dataFn():\n",
    "        idx = utils.get_minibatches_idx(len(data), n_batch, shuffle=shuffle)\n",
    "        for ind in idx:\n",
    "            rows = data.iloc[ind]\n",
    "            yield rows.userId.values, rows.movieId.values, rows.rating.values\n",
    "    return _dataFn\n",
    "\n",
    "# preview training data\n",
    "for u, i, r in dataFn(tr, n_batch=10)():\n",
    "    print(\"users:\\n\", u)\n",
    "    print(\"items:\\n\", i)\n",
    "    print(\"ratings:\\n\", r)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Tensorflow Model Based Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelMF(object):\n",
    "    def __init__(self, user_num, item_num, dim=5, learning_rate=0.1, reg=0.05, modelDir=\"./model/model_mf\"):\n",
    "        \"\"\"初始化 ModelBase Tensorflow Graph.\"\"\"\n",
    "        self.user_num = user_num\n",
    "        self.item_num = item_num\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.isTrain = tf.placeholder(tf.bool, None)\n",
    "                # user data\n",
    "                self.user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "                # item data\n",
    "                self.item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "                # labels\n",
    "                self.rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "            # initFn = tf.contrib.layers.variance_scaling_initializer()\n",
    "            initFn = tf.contrib.layers.xavier_initializer()\n",
    "            # Using a global bias term\n",
    "            self.bias_global = tf.Variable(initFn(shape=[]), name=\"bias_global\")\n",
    "            with tf.variable_scope(\"embedding\"):\n",
    "                # User and item bias variables\n",
    "                self.w_bias_user = tf.Variable(initFn(shape=[user_num]), name=\"embd_bias_user\")\n",
    "                self.w_bias_item = tf.Variable(initFn(shape=[item_num]), name=\"embd_bias_item\")\n",
    "                self.w_user = tf.Variable(initFn(shape=[user_num, dim]), name=\"embd_user\")\n",
    "                self.w_item = tf.Variable(initFn(shape=[item_num, dim]), name=\"embd_item\")\n",
    "                # ------------------------------------------------------------------------------------------\n",
    "                # embedding_lookup: Looks up 'ids' in a list of embedding tensors\n",
    "                # Bias embeddings for user and items, given a batch\n",
    "                # shape of self.bias_user = (user_batch,)\n",
    "                # shape of self.bias_item = (item_batch,)\n",
    "                self.bias_user = tf.nn.embedding_lookup(self.w_bias_user, self.user_batch, name=\"bias_user\")\n",
    "                self.bias_item = tf.nn.embedding_lookup(self.w_bias_item, self.item_batch, name=\"bias_item\")\n",
    "                # Weight embeddings for user and items, given a batch\n",
    "                # shape of self.embd_user = (user_batch * dim)\n",
    "                # shape of self.embd_item = (item_batch * dim)\n",
    "                self.embd_user = tf.nn.embedding_lookup(self.w_user, self.user_batch, name=\"embedding_user\")\n",
    "                self.embd_item = tf.nn.embedding_lookup(self.w_item, self.item_batch, name=\"embedding_item\")\n",
    "\n",
    "            with tf.variable_scope(\"computation\"):\n",
    "                # Do: 若有overfitting後可嘗試dropout\n",
    "                # self.embd_user = tf.layers.dropout(self.embd_user, rate=0.5, training=self.isTrain)\n",
    "                # self.embd_item = tf.layers.dropout(self.embd_item, rate=0.5, training=self.isTrain)\n",
    "                infer = tf.reduce_sum(tf.multiply(self.embd_user, self.embd_item), 1)\n",
    "                infer = tf.add(infer, self.bias_global)\n",
    "                infer = tf.add(infer, self.bias_user)\n",
    "                self.infer = tf.add(infer, self.bias_item, name=\"infer\")\n",
    "\n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                self.regularizer = reg * tf.add(tf.nn.l2_loss(self.embd_user), tf.nn.l2_loss(self.embd_item))\n",
    "                # Do: 嘗試不同的loss function: l2_loss or mse\n",
    "                # l2_loss: Computes half the L2 norm of a tensor without the sqrt => sum(t ** 2) / 2\n",
    "                # self.loss = tf.nn.l2_loss(self.infer - self.rate_batch) + self.regularizer\n",
    "                self.loss = tf.losses.mean_squared_error(labels=self.infer, predictions=self.rate_batch) + self.regularizer\n",
    "                \n",
    "                # for eval\n",
    "                self.rmse_loss = tf.sqrt(tf.reduce_mean(tf.losses.mean_squared_error(labels=self.rate_batch, predictions=self.infer)), name=\"rmse_loss\")\n",
    "                self.mae_loss = tf.reduce_mean(tf.abs(self.infer - self.rate_batch), name=\"mae_loss\")\n",
    "\n",
    "            with tf.variable_scope(\"train\"):\n",
    "                # Do: 嘗試不同的Optimizer\n",
    "                # self.train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "                self.train_op = tf.train.AdagradOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "            self.saver = tf.train.Saver(tf.global_variables())\n",
    "            self.graph = graph\n",
    "            self.modelDir = modelDir\n",
    "\n",
    "    def resetModel(self, modelDir):\n",
    "        \"\"\"刪除model dir\"\"\"\n",
    "        shutil.rmtree(path=modelDir, ignore_errors=True)\n",
    "        os.makedirs(modelDir)\n",
    "\n",
    "    def fit(self, sess, trainGen, testGen, reset=False, nEpoch=50):\n",
    "        \"\"\"model training\"\"\"\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if reset:\n",
    "            print(\"reset model: clean model dir: {} ...\".format(self.modelDir))\n",
    "            self.resetModel(self.modelDir)\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "\n",
    "        start = time.time()\n",
    "        print(\"%s\\t%s\\t%s\\t%s\" % (\"Epoch\", \"Train Error\", \"Val Error\", \"Elapsed Time\"))\n",
    "        minLoss = 1e7\n",
    "        for ep in range(1, nEpoch + 1):\n",
    "            tr_loss, tr_total = 0, 0\n",
    "            for i, (users, items, rates) in enumerate(trainGen(), 1):\n",
    "                loss, _ = sess.run([self.rmse_loss, self.train_op],\n",
    "                                      feed_dict={self.user_batch: users,\n",
    "                                                 self.item_batch: items,\n",
    "                                                 self.rate_batch: rates,\n",
    "                                                 self.isTrain: True})\n",
    "                tr_loss += loss ** 2 * len(users)\n",
    "                tr_total += len(users)\n",
    "                print(\"\\rtrain loss: {:.3f}\".format(loss), end=\"\")\n",
    "            if testGen is not None:\n",
    "                epochLoss = self.epochLoss(sess, testGen)\n",
    "\n",
    "            tpl = \"\\r%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\"\n",
    "            # 紀錄最小valid loss, 若大於之後的loss, 代表model有進步, 才會save model\n",
    "            if minLoss > epochLoss:\n",
    "                tpl += \", saving ...\"\n",
    "                self.saver.save(sess, os.path.join(self.modelDir, 'model'), global_step=ep)\n",
    "                minLoss = epochLoss\n",
    "\n",
    "            end = time.time()\n",
    "            print(tpl % (ep, np.sqrt(tr_loss / tr_total), epochLoss, end - start))\n",
    "            start = end\n",
    "        return self\n",
    "\n",
    "    def ckpt(self, sess, modelDir):\n",
    "        \"\"\"load latest saved model\"\"\"\n",
    "        latestCkpt = tf.train.latest_checkpoint(modelDir)\n",
    "        if latestCkpt:\n",
    "            self.saver.restore(sess, latestCkpt)\n",
    "        return latestCkpt\n",
    "\n",
    "    def epochLoss(self, sess, dataGen, tpe=\"rmse\"):\n",
    "        \"\"\"return RMSE error from all valid data for eval\"\"\"\n",
    "        totLoss, totCnt = 0, 0\n",
    "        for users, items, rates in dataGen():\n",
    "            lossTensor = self.rmse_loss if tpe == \"rmse\" else self.mae_loss\n",
    "            loss = sess.run(lossTensor, feed_dict={self.user_batch: users,\n",
    "                                                   self.item_batch: items,\n",
    "                                                   self.rate_batch: rates,\n",
    "                                                   self.isTrain: False})\n",
    "            totLoss += (loss ** 2 if tpe == \"rmse\" else loss) * len(users)\n",
    "            totCnt += len(users)\n",
    "        return np.sqrt(totLoss / totCnt) if tpe == \"rmse\" else totLoss / totCnt\n",
    "\n",
    "    def predict(self, sess, users: list):\n",
    "        \"\"\"回傳每個user對於所有item的score\"\"\"\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        # ary = np.zeros((len(users), self.item_num))\n",
    "        predAry = []\n",
    "        for u in users:\n",
    "            uBatch = np.repeat(u, self.item_num)\n",
    "            iBatch = np.arange(self.item_num)\n",
    "            pred = sess.run(self.infer,\n",
    "                            feed_dict={self.user_batch: uBatch, self.item_batch: iBatch, self.isTrain: False})\n",
    "            predAry.append(pred)\n",
    "        return np.array(predAry)\n",
    "\n",
    "    def evaluateRMSE(self, sess, dataGen):\n",
    "        \"\"\"計算root mean square error\"\"\"\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return self.epochLoss(sess, dataGen, tpe=\"rmse\")\n",
    "\n",
    "    def evaluateMAE(self, sess, dataGen):\n",
    "        \"\"\"計算 mean absolutely error\"\"\"\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return self.epochLoss(sess, dataGen, tpe=\"mae\")\n",
    "\n",
    "# hyper parameters\n",
    "# Do: 嘗試不同的learning_rate [0.1, 0.001, 0.0001]\n",
    "learning_rate = 0.00001\n",
    "# Do: 嘗試不同的dim [8, 16, 20, 32]\n",
    "dim = 16\n",
    "# Do: 嘗試不同的reg係數 [0.01, 0.005, 0.0001]\n",
    "reg = 0.01\n",
    "# 非必要: 改動model dir\n",
    "modelDir = \"./model/model_mf\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = ModelMF(user_num=nUsers, item_num=nMovies, learning_rate=learning_rate, reg=reg, dim=dim, modelDir=modelDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !cp ./model/model_mf_bak/* ./model/model_mf/\n",
    "n_batch = 128\n",
    "with tf.Session(graph=model.graph,) as sess:\n",
    "    # Do: 參數reset = False可以接續train上次儲存的model, 預設True是清空model dir重來\n",
    "    model.fit(sess, dataFn(tr, n_batch=n_batch), dataFn(te, n_batch=n_batch), nEpoch=20, reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 單一user rating分布圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user id from 0 ~ 670\n",
    "uid = 22\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred = model.predict(sess, [uid])\n",
    "print(\"shape: \", pred.shape, pred)\n",
    "\n",
    "nnzCoord = teRatingMat[uid].nonzero()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"pred distribute\")\n",
    "pd.Series(pred.ravel()[nnzCoord]).hist(bins=30, ax=ax[0])\n",
    "ax[1].set_title(\"real distribute\")\n",
    "pd.Series(map(lambda e: e, teRatingMat[uid][nnzCoord])).hist(bins=30, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    mae_ = model.evaluateMAE(sess, dataFn(te))\n",
    "    rmse_ = model.evaluateRMSE(sess, dataFn(te))\n",
    "\n",
    "print()\n",
    "print(\"MAE loss: \", mae_)\n",
    "print(\"RMSE loss: \", rmse_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User導向評估(Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 可給定user id細看每個user的rating與model預測效果\n",
    "# valid user id from 0 ~ 670\n",
    "uid = 22\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    recomm = model.predict(sess, [uid]).ravel()\n",
    "recommDf = pd.DataFrame(data={\n",
    "              \"userId\": uid,\n",
    "              \"movieId\": range(len(recomm)),\n",
    "              \"title\": midMap[np.arange(len(recomm))].values,\n",
    "              \"rating\": teRatingMat[uid, range(len(recomm))],\n",
    "              \"predRating\": recomm},\n",
    "             columns=(\"userId\", \"movieId\", \"title\", \"rating\", \"predRating\"))\n",
    "# ascending 可以調整True or False觀察結果\n",
    "recommDf.query(\"rating != 0\").sort_values(\"rating\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model導向評估(Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"recommend for user {}\".format(uid))\n",
    "# .query(\"rating != 0\")\n",
    "recommDf.query(\"rating != 0\").sort_values(\"predRating\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_ary, neg_ary = [], []\n",
    "for label in teRatingMat:\n",
    "    label = label[label != 0]\n",
    "    pos_ary.append(sum(label >= 4))\n",
    "    neg_ary.append(sum(label < 4))\n",
    "    # print(\"pos: {}, neg: {}\".format(sum(label >= 4), sum(label < 4)))\n",
    "    \n",
    "def draw_pos_neg(idx):\n",
    "    pd.DataFrame(\n",
    "        index=idx,\n",
    "        data={\"pos\": np.array(pos_ary)[idx], \"neg\": np.array(neg_ary)[idx]}).plot.bar(figsize=(10, 5), alpha=0.8)\n",
    "    plt.show()\n",
    "\n",
    "draw_pos_neg(np.arange(len(teRatingMat))[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Test Data Movie Ratings(觀察上圖)\n",
    "1. 0號, 2號, 5號, 9號 user 正向評價數量 < 10, 就算model全部預測命中, 命中率也不會是 100%!\n",
    "    ex: 0號user只有1個正向評價, 全部命中也指得到0.1的分數\n",
    "2. 3號user正向評價是負向評價的5倍多, 就算亂猜, 中的機率也很高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = sum(np.sum(teRatingMat >= 4, 1) < 10)\n",
    "print(\"{} 個user正向評價總數小於10!\".format(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strict_condition(label):\n",
    "    label = label[label != 0]\n",
    "    pos, neg = sum(label >= 4), sum(label < 4)\n",
    "    return len(label) >= 10 and pos <= neg and pos > 0\n",
    "    \n",
    "print(\"rating數量 >= 10 且 負評價數量 >= 正評價數量 有 [{}] 人\".format(sum(strict_condition(label) for label in teRatingMat)))\n",
    "\n",
    "def norm_condition(label):\n",
    "    label = label[label != 0]\n",
    "    return sum(label >= 4) > 0 and sum(label < 4) > 0\n",
    "\n",
    "print(\"rating正評價數量 >= 0 且 rating負評價數量 >= 0 有 [{}] 人\".format(sum(norm_condition(label) for label in teRatingMat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Precision At K: \n",
    "> **預測分數高(rating >= 4)的前10部電影, 和實際user rating比較, 觀察命中率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def precision_at_k(truth, pred_mat, condition_fn=None, k=10, label_thres=4):\n",
    "    hits, total = 0, 0\n",
    "    for label, pr in zip(truth, pred_mat):\n",
    "        if not condition_fn(label): continue\n",
    "\n",
    "        top_k_ind = (pr * (label != 0)).argsort()[::-1][:k]\n",
    "        hits += sum(label[top_k_ind] >= label_thres)\n",
    "        total += k\n",
    "    return hits / total\n",
    "\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat = []\n",
    "    users = np.arange(nUsers)\n",
    "    for pos in range(0, nUsers, 200):\n",
    "        batch = users[pos:pos + 200]\n",
    "        pred_mat.append(model.predict(sess, batch))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "print( \"strict condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, strict_condition, k=10) )\n",
    "print( \"norm condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, norm_condition, k=10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG: Normalized Discounted Cumulative Gain\n",
    "1. A measure of ranking quality.\n",
    "2. loop 每一位user, prediciton score排序後計算NDCG\n",
    "    <br/>$$ DCG_p = \\sum^p_{i = 1} \\frac{2^{rel_i} - 1}{log_2(i + 1)} $$<br/>\n",
    "3. IDCG: Ideal DCG, 為理想狀態下的DCG分數, 即model全部命中的DCG分數, 而NDCG: Normalized DCG, 公式如下\n",
    "    <br/>$$ NDCG_p = \\sum^p_{i = 1} \\frac{DCG_p}{IDCG_p} $$<br/>\n",
    "4. 所以NDCG是一個比值, 介於0 ~ 1之間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_user_ndcg(label, score, label_thres=4, k=10):\n",
    "    \"\"\"single user ndcg score\"\"\"\n",
    "    nnz = label.nonzero()[0]\n",
    "    # if np.sum(label >= label_thres) < k: return None\n",
    "    label, score = label[nnz], score[nnz]\n",
    "    label = (label >= label_thres).astype(int)\n",
    "    return utils.ndcg_score(label, score, k)\n",
    "\n",
    "def all_user_ndcg(label, pred_mat, cond_fn, label_thres=4, k=10):\n",
    "    \"\"\"avg of all user ndcg score\"\"\"\n",
    "    tot_ndcg, actual_cnt = 0, 0\n",
    "    for i, (label, score) in enumerate(zip(teRatingMat, pred_mat)):\n",
    "        if not cond_fn(label): continue\n",
    "\n",
    "        ndcg = single_user_ndcg(label, score, k=10)\n",
    "        if ndcg is not None:\n",
    "            tot_ndcg += ndcg\n",
    "            actual_cnt += 1\n",
    "    return tot_ndcg / actual_cnt\n",
    "\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat = []\n",
    "    users = np.arange(nUsers)\n",
    "    for pos in range(0, nUsers, 200):\n",
    "        batch = users[pos:pos + 200]\n",
    "        pred_mat.append(model.predict(sess, batch))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "strict_ndcg = all_user_ndcg(teRatingMat, pred_mat, strict_condition, label_thres=4, k=10)\n",
    "norm_ndcg = all_user_ndcg(teRatingMat, pred_mat, norm_condition, label_thres=4, k=10)\n",
    "print(\"strict condition ndcg at 10: \", strict_ndcg)\n",
    "print(\"norm condition ndcg at 10: \", norm_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall At K: \n",
    "> **對於每個user喜歡的前10部電影中(rating >= 4), 和預測值比較, 觀察命中率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def recall_at_k(truth, pred_mat, condition_fn, k=10, label_thres=4, pred_thres=0.8):\n",
    "    hits, total = 0, 0\n",
    "    for labels, pr in zip(truth, pred_mat):\n",
    "        if not condition_fn(labels): continue\n",
    "\n",
    "        top_percentile = np.percentile(pr, pred_thres * 100)\n",
    "        top_k_ind = labels.argsort()[::-1][:k]\n",
    "        hits += sum(pr[top_k_ind] >= top_percentile)\n",
    "        # hits += recall_score(labels >= label_thres, pr >= np.percentile(pr, pred_thres * 100))\n",
    "        total += k\n",
    "    return hits / total\n",
    "\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat = []\n",
    "    users = np.arange(nUsers)\n",
    "    for pos in range(0, nUsers, 200):\n",
    "        batch = users[pos:pos + 200]\n",
    "        pred_mat.append(model.predict(sess, batch))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "def recall_cond_fn(label):\n",
    "    \"\"\"recall條件限制為, user在test data正向評價電影至少10部\"\"\"\n",
    "    return sum(label >= 4) >= 10\n",
    "    \n",
    "print( recall_at_k(teRatingMat, pred_mat, condition_fn=recall_cond_fn) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC_CURVE (Receiver operating characteristic), AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "def drawRocCurve(y, predProba):\n",
    "    fprRf, tprRf, _ = roc_curve(y, predProba, pos_label=1)\n",
    "    aucScr = auc(fprRf, tprRf)\n",
    "    print(\"auc:\", aucScr)\n",
    "    f, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.plot(fprRf, tprRf, label='ROC CURVE')\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.set_title('AOC: Area Under Curve (score: {:.4f})'.format(aucScr))\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "# 只看非0的部分\n",
    "coord = teRatingMat.nonzero()\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    predMat = model.predict(sess, np.arange(teRatingMat.shape[0]).tolist())\n",
    "    predMat = predMat / predMat.max()\n",
    "    \n",
    "# regard rating >= 4 as user like this movie\n",
    "drawRocCurve((teRatingMat[coord] >= 4).astype(int), predMat[coord])\n",
    "# ind = predMat[coord].argsort()\n",
    "# drawRocCurve((teRatingMat[coord] >= 4).astype(int)[ind], predMat[coord][ind[::-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Baseline: 不經過訓練隨機產生預測值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummyPredMat = np.random.random((nUsers, nMovies))\n",
    "drawRocCurve((teRatingMat >= 4).astype(int).ravel(), dummyPredMat.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 不經過訓練隨機產生預測值 precision at 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print( \"strict condition precision at 10: \", precision_at_k(teRatingMat, dummyPredMat, strict_condition, k=10) )\n",
    "print( \"norm condition precision at 10: \", precision_at_k(teRatingMat, dummyPredMat, norm_condition, k=10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 不經過訓練隨機產生預測值 ndcg at 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strict_ndcg = all_user_ndcg(label, dummyPredMat, strict_condition, label_thres=4, k=10)\n",
    "norm_ndcg = all_user_ndcg(label, dummyPredMat, norm_condition, label_thres=4, k=10)\n",
    "print(\"strict condition ndcg at 10: \", strict_ndcg)\n",
    "print(\"norm condition ndcg at 10: \", norm_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## 取出movies embedding, 使用cosine similarity列出最相似的電影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies[movies.title.str.contains(\"Toy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def most_like(model, seed_movie, k=10):\n",
    "    with tf.Session(graph=model.graph) as sess:\n",
    "        model.ckpt(sess, model.modelDir)\n",
    "        movie_emb = sess.run(model.embd_item, feed_dict={model.item_batch: movies.movieId.values})\n",
    "        \n",
    "    most_like = cosine_similarity(movie_emb[seed_movie][np.newaxis, :], movie_emb).ravel().argsort()[::-1][:k]\n",
    "    return movies.iloc[most_like]\n",
    "\n",
    "most_like(model, 0, k=10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
