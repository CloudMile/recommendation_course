{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, re, codecs, json, time\n",
    "import pickle, collections, random, math, numbers, scipy.sparse as sp, itertools, shutil\n",
    "\n",
    "def reload(mName):\n",
    "    import importlib\n",
    "    if mName in sys.modules:\n",
    "        del sys.modules[mName]\n",
    "    return importlib.import_module(mName)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict, Counter\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "utils = reload('utils.utils')\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "randomSeed = 88\n",
    "np.random.seed(randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Data Prepare\n",
    "- mapping user id, movie id to zero start indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"./data/ml-latest-small/ratings.csv\")\n",
    "movies = pd.read_csv(\"./data/ml-latest-small/movies.csv\")\n",
    "tags = pd.read_csv(\"./data/ml-latest-small/tags.csv\")\n",
    "\n",
    "uidEnc, midEnc = LabelEncoder(), LabelEncoder()\n",
    "# encode user id and movie id to real value\n",
    "midEnc.fit(movies.movieId)\n",
    "uidEnc.fit(ratings.userId)\n",
    "\n",
    "ratings[\"userId\"] = uidEnc.transform(ratings.userId)\n",
    "ratings[\"movieId\"] = midEnc.transform(ratings.movieId)\n",
    "\n",
    "movies[\"movieId\"] = midEnc.transform(movies.movieId)\n",
    "\n",
    "tags[\"userId\"] = uidEnc.transform(tags.userId)\n",
    "tags[\"movieId\"] = midEnc.transform(tags.movieId)\n",
    "\n",
    "midMap = pd.Series(dict(zip(movies.movieId, movies.title)))\n",
    "\n",
    "nUsers, nMovies = len(uidEnc.classes_), len(midEnc.classes_)\n",
    "\n",
    "print(ratings.shape)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies profile\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags profile\n",
    "tags.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "### Split Train and Test Data\n",
    "1. 拆分train test data\n",
    "2. number of user = 671, number of movies = 9125 => movie數量越大於user數量則越貼近現實\n",
    "3. 組合interaction matrix of train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 已經產生出檔案的不用跑這一段\n",
    "# tr, te = utils.split_ratings(ratings, testRatio=0.3)\n",
    "# tr.to_csv(\"./data/ml-latest-small/movielens.tr.csv\", index=False)\n",
    "# te.to_csv(\"./data/ml-latest-small/movielens.te.csv\", index=False)\n",
    "# utils.dumpPickle(\"./data/ml-latest-small/state.h\", \n",
    "#         {\"uidEnc\": uidEnc, \"midEnc\": midEnc, \"midMap\": midMap, \"nUsers\": nUsers, \"nMovies\":nMovies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv(\"./data/ml-latest-small/movielens.tr.csv\")\n",
    "te = pd.read_csv(\"./data/ml-latest-small/movielens.te.csv\")\n",
    "\n",
    "# state = utils.loadPickle(\"./data/ml-latest-small/state.h\")\n",
    "# uidEnc, midEnc, midMap, nUsers, nMovies = \\\n",
    "#     (state[\"uidEnc\"], state[\"midEnc\"], state[\"midMap\"], state[\"nUsers\"], state[\"nMovies\"])\n",
    "\n",
    "# train data rating matrix\n",
    "trRatingMat = np.zeros((nUsers, nMovies))\n",
    "# test data rating matrix\n",
    "teRatingMat = np.zeros((nUsers, nMovies))\n",
    "for idx, r in tr.iterrows():\n",
    "    trRatingMat[int(r.userId), int(r.movieId)] = r.rating\n",
    "for idx, r in te.iterrows():\n",
    "    teRatingMat[int(r.userId), int(r.movieId)] = r.rating\n",
    "\n",
    "print(\"train interaction matrix shape: \", trRatingMat.shape, \"test interaction matrix shape: \", teRatingMat.shape)\n",
    "print(\"train.shape: \", tr.shape, \"test.shape: \", te.shape)\n",
    "print()\n",
    "print(tr.head())\n",
    "print()\n",
    "print(te.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以leave one out方式產生 train data, test data\n",
    "1. 每一筆資料有兩部分: [user query] + [item meta]\n",
    "2. movie多加了兩個欄位: avg_rating, year\n",
    "    - avg_rating是所有user對於同一部電影的平均評分\n",
    "    - year則是從title extract出來的資訊\n",
    "3. 每一筆user query 包含所有user movie history, 除了當前的rating movie (candidate movie)\n",
    "4. test data的user query來自於train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data, movie_trans, train_hist=None, is_train=True):\n",
    "    queue = []\n",
    "    data = data.merge(movie_trans, how=\"left\", on=\"movieId\")\n",
    "    columns=[\"user_id\", \"query_movie_ids\", \n",
    "             \"genres\", \"avg_rating\", \"year\", \"candidate_movie_id\",\n",
    "             \"rating\"]\n",
    "    for u, df in data.groupby(\"userId\"):\n",
    "        df = df.sort_values(\"rating\", ascending=False)\n",
    "        if not is_train:\n",
    "            user_movies_hist = train_hist.query(\"userId == {}\".format(u)).movieId\n",
    "        for i, (_, r) in enumerate(df.iterrows()):\n",
    "            if is_train:\n",
    "                queue.append([int(r.userId), df.movieId[:i].tolist() + df.movieId[i + 1:].tolist(), r.genres, r.avg_rating, r.year, int(r.movieId), r.rating])\n",
    "            else:\n",
    "                # queue.append([int(r.userId), df.movieId[:i].tolist() + df.movieId[i + 1:].tolist(), r.genres, r.avg_rating, r.year, int(r.movieId), r.rating])\n",
    "                all_hist = set(user_movies_hist.tolist() + df.movieId[:i].tolist())\n",
    "                queue.append([int(r.userId), list(all_hist - set([int(r.movieId)])), r.genres, r.avg_rating, r.year, int(r.movieId), r.rating])\n",
    "    return pd.DataFrame(queue, columns=columns)\n",
    "\n",
    "movie_trans, genres_enc = utils.doMovies(movies)\n",
    "movie_trans[\"avg_rating\"] = ratings.groupby(\"movieId\").rating.mean()\n",
    "movie_trans[\"avg_rating\"] = minmax_scale(movie_trans.avg_rating.fillna(ratings.rating.mean()))\n",
    "movie_trans[\"year\"] = movie_trans.title.str.findall(\"\\(\\s*(\\d+)\\s*\\)\").map(lambda lst: int(lst[-1]) if len(lst) else None)\n",
    "movie_trans[\"year\"] = minmax_scale(movie_trans.year.fillna(movie_trans.year.median()))\n",
    "n_genres = len(genres_enc.enc)\n",
    "\n",
    "trProcessed = preprocess(tr, movie_trans)\n",
    "teProcessed = preprocess(te, movie_trans, tr, is_train=False)\n",
    "trProcessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teProcessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Function\n",
    "1. 由於tensorflow placeholder不支援變動長度的columns, 需透過padding zero(補零)帶入\n",
    "2. 每個變動長度的column, 需要再給lens描述每一筆資料的長度, ex: genres, genres_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_multi(df, multi_cols):\n",
    "    \"\"\"對於multivalent的欄位, 需要增加一個column去描述該欄位的長度\"\"\"\n",
    "    pad = tf.keras.preprocessing.sequence.pad_sequences\n",
    "    ret = OrderedDict()\n",
    "    for colname, col in df.iteritems():\n",
    "        if colname in multi_cols:\n",
    "            lens = col.map(len)\n",
    "            ret[colname] = list(pad(col, padding=\"post\", maxlen=lens.max()))\n",
    "            ret[colname + \"_len\"] = lens.values\n",
    "        else:\n",
    "            ret[colname] = col.values\n",
    "    return ret\n",
    "\n",
    "def dataFn(data, n_batch=128, shuffle=False):\n",
    "    pad = tf.keras.preprocessing.sequence.pad_sequences\n",
    "    def fn():\n",
    "        dataInner = data.copy()\n",
    "        indices = utils.get_minibatches_idx(len(dataInner), n_batch, shuffle=shuffle)\n",
    "        for ind in indices:\n",
    "            yield do_multi(dataInner.iloc[ind], [\"query_movie_ids\", \"genres\"])\n",
    "    return fn\n",
    "\n",
    "for i, e in enumerate(dataFn(trProcessed, n_batch=5, shuffle=True)(), 1):\n",
    "    # print(e)\n",
    "    break\n",
    "pd.DataFrame(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF + DNN with Pointwise Loss Function (RMSE or L2 loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ModelMfDNN(object):\n",
    "    def __init__(self,\n",
    "                 n_items,\n",
    "                 n_genres,\n",
    "                 dim=32,\n",
    "                 learning_rate=0.01,\n",
    "                 modelDir=\"./model/model_mf_with_dnn\"):\n",
    "        \"\"\"初始化 Tensorflow Graph\"\"\"\n",
    "        self.n_items = n_items\n",
    "        self.n_genres = n_genres\n",
    "        self.ftr_cols = OrderedDict()\n",
    "\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            # inputs/id_user:0\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.isTrain = tf.placeholder(tf.bool, None)\n",
    "                # user data\n",
    "                self.user_id = tf.placeholder(tf.int32, [None])\n",
    "                self.query_movie_ids = tf.placeholder(tf.int32, [None, None])\n",
    "                self.query_movie_ids_len = tf.placeholder(tf.int32, [None])\n",
    "                # item data\n",
    "                self.genres = tf.placeholder(tf.int32, [None, None])\n",
    "                self.genres_len = tf.placeholder(tf.int32, [None])\n",
    "                self.avg_rating = tf.placeholder(tf.float32, [None])\n",
    "                self.year = tf.placeholder(tf.float32, [None])\n",
    "                self.candidate_movie_id = tf.placeholder(tf.int32, [None])\n",
    "                self.rating = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "            init_fn = tf.glorot_normal_initializer()\n",
    "            emb_init_fn = tf.glorot_uniform_initializer()\n",
    "            self.b_global = tf.Variable(emb_init_fn(shape=[]), name=\"b_global\")\n",
    "            with tf.variable_scope(\"embedding\"):\n",
    "                self.w_query_movie_ids = tf.Variable(emb_init_fn(shape=[self.n_items, dim]), name=\"w_query_movie_ids\")\n",
    "                self.b_query_movie_ids = tf.Variable(emb_init_fn(shape=[dim]), name=\"b_query_movie_ids\")\n",
    "                self.w_candidate_movie_id = tf.Variable(init_fn(shape=[self.n_items, dim]), name=\"w_candidate_movie_id\")\n",
    "                self.b_candidate_movie_id = tf.Variable(init_fn(shape=[dim + 8 + 2]), name=\"b_candidate_movie_id\")\n",
    "                self.w_genres = tf.Variable(emb_init_fn(shape=[self.n_genres, 8]), name=\"w_genres\")\n",
    "\n",
    "                # query_movie embedding\n",
    "                self.query_emb = tf.nn.embedding_lookup(self.w_query_movie_ids, self.query_movie_ids)\n",
    "                query_movie_mask = tf.expand_dims(tf.nn.l2_normalize(tf.to_float(tf.sequence_mask(self.query_movie_ids_len)), 1), -1)\n",
    "                self.query_emb = tf.reduce_sum(self.query_emb * query_movie_mask, 1)\n",
    "                self.query_bias = tf.matmul(self.query_emb, self.b_query_movie_ids[:, tf.newaxis])\n",
    "\n",
    "                # candidate_movie embedding\n",
    "                self.candidate_emb = tf.nn.embedding_lookup(self.w_candidate_movie_id, self.candidate_movie_id)\n",
    "\n",
    "                # genres embedding\n",
    "                self.genres_emb = tf.nn.embedding_lookup(self.w_genres, tf.to_int32(self.genres))\n",
    "                genres_mask = tf.expand_dims( tf.nn.l2_normalize(tf.to_float(tf.sequence_mask( tf.reshape(self.genres_len, [-1])) ), 1), -1)\n",
    "                self.genres_emb = tf.reduce_sum(self.genres_emb * genres_mask, 1)\n",
    "            \n",
    "            with tf.variable_scope(\"dnn\"):\n",
    "                # encode [item embedding + item metadata]\n",
    "                self.item_repr = tf.concat([self.candidate_emb, self.genres_emb, self.avg_rating[:, tf.newaxis], self.year[:, tf.newaxis]], 1)\n",
    "                self.candidate_bias = tf.matmul(self.item_repr, self.b_candidate_movie_id[:, tf.newaxis])\n",
    "                \n",
    "                # Do: 若有overfitting後可嘗試dropout\n",
    "                # Do: 目前兩層DNN, 可以試著增加或減少hidden layer, 但是最後要跟query_emb內積, 最後一層維度必須 = dim\n",
    "                # Example:\n",
    "                    # self.item_repr = tf.layers.dense(self.item_repr, 64, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                    # self.item_repr = tf.layers.dense(self.item_repr, 32, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                    # self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                # dp_scale = 0.5\n",
    "                self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                # self.item_repr = tf.layers.dropout(self.item_repr, dp_scale, training=self.isTrain)\n",
    "                self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                # self.item_repr = tf.layers.dropout(self.item_repr, dp_scale, training=self.isTrain)\n",
    "                \n",
    "                \n",
    "            with tf.variable_scope(\"computation\"):\n",
    "                infer = tf.reduce_sum(self.query_emb * self.item_repr, 1, keep_dims=True)\n",
    "                infer = tf.add(infer, self.b_global)\n",
    "                infer = tf.add(infer, self.query_bias)\n",
    "                self.infer = tf.add(infer, self.candidate_bias, name=\"infer\")\n",
    "\n",
    "                # one query for all items\n",
    "                self.pred = tf.matmul(self.query_emb, tf.transpose(self.item_repr)) + \\\n",
    "                            tf.reshape(self.candidate_bias, (1, -1)) + self.query_bias + self.b_global\n",
    "                pass\n",
    "\n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                self.loss = tf.losses.mean_squared_error(labels=self.rating[:, tf.newaxis], predictions=self.infer)\n",
    "\n",
    "                # for eval\n",
    "                self.rmse_loss = tf.sqrt(self.loss)\n",
    "                self.mae_loss = tf.reduce_mean(tf.abs(self.infer - self.rating[:, tf.newaxis]))\n",
    "                pass\n",
    "\n",
    "            with tf.variable_scope(\"train\"):\n",
    "                # Do: 嘗試不同的Optimizer\n",
    "                self.train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "                # self.train_op = tf.train.AdagradOptimizer(learning_rate).minimize(self.loss)\n",
    "                # self.train_op = tf.train.RMSPropOptimizer(learning_rate).minimize(self.loss)\n",
    "                # self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "                pass\n",
    "\n",
    "            self.saver = tf.train.Saver(tf.global_variables())\n",
    "            self.graph = graph\n",
    "            self.modelDir = modelDir\n",
    "\n",
    "    def resetModel(self, modelDir):\n",
    "        shutil.rmtree(path=modelDir, ignore_errors=True)\n",
    "        os.makedirs(modelDir)\n",
    "\n",
    "    def feed_dict(self, data, mode=\"train\"):\n",
    "        ret = {\n",
    "            self.user_id: data[\"user_id\"],\n",
    "            self.query_movie_ids: data[\"query_movie_ids\"],\n",
    "            self.query_movie_ids_len: data[\"query_movie_ids_len\"],\n",
    "            self.genres: data[\"genres\"],\n",
    "            self.genres_len: data[\"genres_len\"],\n",
    "            self.avg_rating: data[\"avg_rating\"],\n",
    "            self.year: data[\"year\"],\n",
    "            self.candidate_movie_id: data[\"candidate_movie_id\"]\n",
    "        }\n",
    "        ret[self.isTrain] = False\n",
    "        if mode != \"infer\":\n",
    "            ret[self.rating] = data[\"rating\"]\n",
    "            if mode == \"train\":\n",
    "                ret[self.isTrain] = True\n",
    "            elif mode == \"eval\":\n",
    "                pass\n",
    "        return ret\n",
    "\n",
    "    def fit(self, sess, trainGen, testGen, reset=False, nEpoch=50):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if reset:\n",
    "            print(\"reset model: clean model dir: {} ...\".format(self.modelDir))\n",
    "            self.resetModel(self.modelDir)\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "\n",
    "        start = time.time()\n",
    "        print(\"%s\\t%s\\t%s\\t%s\" % (\"Epoch\", \"Train Error\", \"Val Error\", \"Elapsed Time\"))\n",
    "        minLoss = 1e7\n",
    "        for ep in range(1, nEpoch + 1):\n",
    "            tr_loss, tr_total = 0, 0\n",
    "            for i, data in enumerate(trainGen(), 1):\n",
    "                loss, _ = sess.run([self.rmse_loss, self.train_op], feed_dict=self.feed_dict(data, mode=\"train\"))\n",
    "                tr_loss += loss ** 2 * len(data[\"query_movie_ids\"])\n",
    "                tr_total += len(data[\"query_movie_ids\"])\n",
    "                print(\"\\rtrain loss: {:.3f}\".format(loss), end=\"\")\n",
    "            if testGen is not None:\n",
    "                epochLoss = self.epochLoss(sess, testGen)\n",
    "\n",
    "            tpl = \"\\r%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\"\n",
    "            if minLoss > epochLoss:\n",
    "                tpl += \", saving ...\"\n",
    "                self.saver.save(sess, os.path.join(self.modelDir, 'model'), global_step=ep)\n",
    "                minLoss = epochLoss\n",
    "\n",
    "            end = time.time()\n",
    "            print(tpl % (ep, np.sqrt(tr_loss / tr_total), epochLoss, end - start))\n",
    "            start = end\n",
    "        return self\n",
    "\n",
    "    def ckpt(self, sess, modelDir):\n",
    "        \"\"\"load latest saved model\"\"\"\n",
    "        latestCkpt = tf.train.latest_checkpoint(modelDir)\n",
    "        if latestCkpt:\n",
    "            self.saver.restore(sess, latestCkpt)\n",
    "        return latestCkpt\n",
    "\n",
    "    def epochLoss(self, sess, dataGen, tpe=\"rmse\"):\n",
    "        totLoss, totCnt = 0, 0\n",
    "        for data in dataGen():\n",
    "            lossTensor = self.rmse_loss if tpe == \"rmse\" else self.mae_loss\n",
    "            loss = sess.run(lossTensor, feed_dict=self.feed_dict(data, mode=\"eval\"))\n",
    "            totLoss += (loss ** 2 if tpe == \"rmse\" else loss) * len(data[\"query_movie_ids\"])\n",
    "            totCnt += len(data[\"query_movie_ids\"])\n",
    "        return np.sqrt(totLoss / totCnt) if tpe == \"rmse\" else totLoss / totCnt\n",
    "\n",
    "    def predict(self, sess, user_queries, items):\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return sess.run(self.pred, feed_dict={\n",
    "            self.isTrain: False,\n",
    "            self.user_id: user_queries[\"user_id\"],\n",
    "            self.query_movie_ids: user_queries[\"query_movie_ids\"],\n",
    "            self.query_movie_ids_len: user_queries[\"query_movie_ids_len\"],\n",
    "\n",
    "            self.genres: items[\"genres\"],\n",
    "            self.genres_len: items[\"genres_len\"],\n",
    "            self.avg_rating: items[\"avg_rating\"],\n",
    "            self.year: items[\"year\"],\n",
    "            self.candidate_movie_id: items[\"candidate_movie_id\"]\n",
    "        })\n",
    "\n",
    "    def evaluateRMSE(self, sess, dataGen):\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return self.epochLoss(sess, dataGen, tpe=\"rmse\")\n",
    "\n",
    "    def evaluateMAE(self, sess, dataGen):\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return self.epochLoss(sess, dataGen, tpe=\"mae\")\n",
    "\n",
    "# hyper parameters\n",
    "# Do: 嘗試不同的learning_rate [0.1, 0.001, 0.0001]\n",
    "learning_rate = 0.0001\n",
    "# Do: 嘗試不同的dim [8, 16, 20, 32]\n",
    "dim = 16\n",
    "# 非必要: 改動model dir\n",
    "modelDir = \"./model/model_mf_with_dnn\"\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "model = ModelMfDNN(\n",
    "            n_items=nMovies,\n",
    "            n_genres=n_genres,\n",
    "            dim=dim,\n",
    "            learning_rate=learning_rate,\n",
    "            modelDir=modelDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !cp ./model/model_mf_with_dnn_bak/* ./model/model_mf_with_dnn/\n",
    "# 每個batch的數量\n",
    "n_batch = 128\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    # Do: 參數reset = False可以接續train上次儲存的model, 預設True是清空model dir重來\n",
    "    model.fit(sess, dataFn(trProcessed, n_batch=n_batch, shuffle=True), dataFn(teProcessed, n_batch=n_batch), nEpoch=10, reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Data Function For Predict\n",
    "1. For fast prediction, make another function\n",
    "2. 原始的dataFn是 user meta + item meta, 若預測671 users對上9125 items的評分需要671 x 9125筆資料, serving time會太久\n",
    "3. 現在預測所有users對所有items的資料只需要671 + 9125筆資料, 執行graph中的pred節點即可\n",
    "   (請在ModelMfDNN的 __init__ method 找 self.pred operation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# columns = [\"user_id\", \"query_movie_ids\", \"query_movie_ids_len\",\n",
    "#            \"genres\", \"genres_len\", \"avg_rating\", \"year\", \"candidate_movie_id\",\n",
    "#            \"rating\"]\n",
    "def user_item_data(data, uids, movie_trans, n_batch=128):\n",
    "    u_col = [\"user_id\", \"query_movie_ids\"]\n",
    "    cache = {\"u_ary\": []}\n",
    "    items = do_multi(movie_trans, [\"genres\"])\n",
    "    items[\"candidate_movie_id\"] = items.pop(\"movieId\")\n",
    "    def clear(u_ary):\n",
    "        u_data = do_multi(pd.DataFrame(data=u_ary, columns=u_col), [\"query_movie_ids\"])\n",
    "        cache[\"u_ary\"] = []\n",
    "        return u_data\n",
    "    \n",
    "    for uid, df in data[data.user_id.isin(uids)].groupby(\"user_id\"):\n",
    "        u_rec, u_ary = df.iloc[0], cache[\"u_ary\"]\n",
    "        # print(u_rec.query_movie_ids, u_rec.candidate_movie_id)\n",
    "        u_rec.set_value(\"query_movie_ids\", u_rec.query_movie_ids + [u_rec.candidate_movie_id])\n",
    "        u_ary.append(u_rec[u_col].values)\n",
    "        if len(u_ary) >= n_batch:\n",
    "            yield clear(u_ary), items\n",
    "    yield clear(u_ary), items\n",
    "    \n",
    "for u_data, items in user_item_data(trProcessed, np.arange(0, 5), movie_trans, n_batch=5):\n",
    "    break\n",
    "pd.DataFrame(u_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(items).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 單一user rating分布圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# user id from 0 ~ 670\n",
    "uid = 22\n",
    "u_queries, movies_meta = list(user_item_data(trProcessed, [uid], movie_trans))[0]\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred = model.predict(sess, u_queries, movies_meta)\n",
    "print(\"shape: \", pred.shape, minmax_scale(pred.T).T)\n",
    "\n",
    "nnzCoord = teRatingMat[uid].nonzero()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"pred distribution\")\n",
    "pd.Series(pred.ravel()[nnzCoord]).hist(bins=30, ax=ax[0])\n",
    "ax[1].set_title(\"real distribution\")\n",
    "pd.Series(map(lambda e: e, teRatingMat[uid][nnzCoord])).hist(bins=30, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_batch = 128\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    mae_ = model.evaluateMAE(sess, dataFn(teProcessed, n_batch=n_batch, shuffle=False))\n",
    "    rmse_ = model.evaluateRMSE(sess, dataFn(teProcessed, n_batch=n_batch, shuffle=False))\n",
    "\n",
    "print()\n",
    "print(\"MAE loss: \", mae_)\n",
    "print(\"RMSE loss: \", rmse_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User導向評估(Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 可給定user id細看每個user的rating與model預測效果\n",
    "# valid user id from 0 ~ 670\n",
    "uid = 26\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    u_queries, movies_meta = list(user_item_data(trProcessed, [uid], movie_trans))[0]\n",
    "    recomm = model.predict(sess, u_queries, movies_meta).ravel()\n",
    "recommDf = pd.DataFrame(data={\n",
    "              \"userId\": uid,\n",
    "              \"movieId\": range(len(recomm)), \n",
    "              \"title\": midMap[np.arange(len(recomm))].values, \n",
    "              \"rating\": teRatingMat[uid, range(len(recomm))],\n",
    "              \"predRating\": recomm},\n",
    "             columns=(\"userId\", \"movieId\", \"title\", \"rating\", \"predRating\"))\n",
    "# ascending 可以調整True or False觀察結果\n",
    "recommDf.query(\"rating != 0\").sort_values(\"rating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model導向評估(Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# .query(\"rating != 0\")\n",
    "recommDf.query(\"rating != 0\").sort_values(\"predRating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC_CURVE (Receiver operating characteristic), AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def drawRocCurve(y, predProba):\n",
    "    fprRf, tprRf, _ = roc_curve(y, predProba, pos_label=1)\n",
    "    aucScr = auc(fprRf, tprRf)\n",
    "    print(\"auc:\", aucScr)\n",
    "    f, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.plot(fprRf, tprRf, label='ROC CURVE')\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.set_title('AOC: Area Under Curve (score: {:.4f})'.format(aucScr))\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "coord = teRatingMat.nonzero()\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    predMat = []\n",
    "    for u_data, items in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=128):\n",
    "        predMat.append(model.predict(sess, u_data, items))\n",
    "        # predMat.append(model.predict(sess, u_data))\n",
    "    predMat = np.vstack(predMat)\n",
    "# regard rating >= 4 as user like this movie\n",
    "drawRocCurve((teRatingMat[coord] >= 4).astype(int), predMat[coord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pos_ary, neg_ary = [], []\n",
    "for label in teRatingMat:\n",
    "    label = label[label != 0]\n",
    "    pos_ary.append(sum(label >= 4))\n",
    "    neg_ary.append(sum(label < 4))\n",
    "    # print(\"pos: {}, neg: {}\".format(sum(label >= 4), sum(label < 4)))\n",
    "    \n",
    "def draw_pos_neg(idx):\n",
    "    pd.DataFrame(\n",
    "        index=idx,\n",
    "        data={\"pos\": np.array(pos_ary)[idx], \"neg\": np.array(neg_ary)[idx]}).plot.bar(figsize=(10, 5), alpha=0.8)\n",
    "    plt.show()\n",
    "\n",
    "# 可調整index觀察各user rating分布 ex: [0:10] [10:20] [600:610]\n",
    "draw_pos_neg(np.arange(len(teRatingMat))[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Test Data Movie Ratings(觀察上圖)\n",
    "1. 0號, 2號, 5號, 9號 user 正向評價數量 < 10, 就算model全部預測命中, 命中率也不會是 100%!\n",
    "    ex: 0號user只有1個正向評價, 全部命中也指得到0.1的分數\n",
    "2. 3號user正向評價是負向評價的5倍多, 就算亂猜, 中的機率也很高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = sum(np.sum(teRatingMat >= 4, 1) < 10)\n",
    "print(\"{} 個user正向評價總數小於10!\".format(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_condition(label):\n",
    "    label = label[label != 0]\n",
    "    pos, neg = sum(label >= 4), sum(label < 4)\n",
    "    return len(label) >= 10 and pos <= neg and pos > 0\n",
    "    \n",
    "print(\"rating數量 >= 10 且 負評價數量 >= 正評價數量 有 [{}] 人\".format(sum(strict_condition(label) for label in teRatingMat)))\n",
    "\n",
    "def norm_condition(label):\n",
    "    label = label[label != 0]\n",
    "    return sum(label >= 4) > 0 and sum(label < 4) > 0\n",
    "\n",
    "print(\"rating正評價數量 >= 0 且 rating負評價數量 >= 0 有 [{}] 人\".format(sum(norm_condition(label) for label in teRatingMat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision At K: \n",
    "> **預測分數高(rating >= 4)的前10部電影, 和實際user rating比較, 觀察命中率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def precision_at_k(truth, pred_mat, condition_fn=None, k=10, label_thres=4):\n",
    "    hits, total = 0, 0\n",
    "    for label, pr in zip(truth, pred_mat):\n",
    "        if not condition_fn(label): continue\n",
    "\n",
    "        top_k_ind = (pr * (label != 0)).argsort()[::-1][:k]\n",
    "        hits += sum(label[top_k_ind] >= label_thres)\n",
    "        total += k\n",
    "    return hits / total\n",
    "\n",
    "n_batch = 128\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, i_data in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=n_batch):\n",
    "        pred_mat.append(model.predict(sess, u_data, i_data))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "print( \"strict condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, strict_condition, k=10) )\n",
    "print( \"norm condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, norm_condition, k=10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG: Normalized Discounted Cumulative Gain\n",
    "1. A measure of ranking quality.\n",
    "2. loop 每一位user, prediciton score排序後計算NDCG\n",
    "    <br/>$$ DCG_p = \\sum^p_{i = 1} \\frac{2^{rel_i} - 1}{log_2(i + 1)} $$<br/>\n",
    "3. IDCG: Ideal DCG, 為理想狀態下的DCG分數, 即model全部命中的DCG分數, 而NDCG: Normalized DCG, 公式如下\n",
    "    <br/>$$ NDCG_p = \\sum^p_{i = 1} \\frac{DCG_p}{IDCG_p} $$<br/>\n",
    "4. 所以NDCG是一個比值, 介於0 ~ 1之間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def single_user_ndcg(label, score, label_thres=4, k=10):\n",
    "    \"\"\"single user ndcg score\"\"\"\n",
    "    nnz = label.nonzero()[0]\n",
    "    # if np.sum(label >= label_thres) < k: return None\n",
    "    label, score = label[nnz], score[nnz]\n",
    "    label = (label >= label_thres).astype(int)\n",
    "    return utils.ndcg_score(label, score, k)\n",
    "\n",
    "def all_user_ndcg(label, pred_mat, cond_fn, label_thres=4, k=10):\n",
    "    \"\"\"avg of all user ndcg score\"\"\"\n",
    "    tot_ndcg, actual_cnt = 0, 0\n",
    "    for i, (label, score) in enumerate(zip(teRatingMat, pred_mat)):\n",
    "        if not cond_fn(label): continue\n",
    "\n",
    "        ndcg = single_user_ndcg(label, score, k=10)\n",
    "        if ndcg is not None:\n",
    "            tot_ndcg += ndcg\n",
    "            actual_cnt += 1\n",
    "    return tot_ndcg / actual_cnt\n",
    "\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, items in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=128):\n",
    "        pred_mat.append(model.predict(sess, u_data, items))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "strict_ndcg = all_user_ndcg(teRatingMat, pred_mat, strict_condition, label_thres=4, k=10)\n",
    "norm_ndcg = all_user_ndcg(teRatingMat, pred_mat, norm_condition, label_thres=4, k=10)\n",
    "print(\"strict condition ndcg at 10: \", strict_ndcg)\n",
    "print(\"norm condition ndcg at 10: \", norm_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall At K: \n",
    "> **對於每個user喜歡的前10部電影中(rating >= 4), 和預測值比較, 觀察命中率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def recall_at_k(truth, pred_mat, condition_fn, k=10, label_thres=4, pred_thres=0.8):\n",
    "    hits, total = 0, 0\n",
    "    for labels, pr in zip(truth, pred_mat):\n",
    "        if not condition_fn(labels): continue\n",
    "\n",
    "        top_percentile = np.percentile(pr, pred_thres * 100)\n",
    "        top_k_ind = labels.argsort()[::-1][:k]\n",
    "        hits += sum(pr[top_k_ind] >= top_percentile)\n",
    "        # hits += recall_score(labels >= label_thres, pr >= np.percentile(pr, pred_thres * 100))\n",
    "        total += k\n",
    "    return hits / total\n",
    "    \n",
    "n_batch = 128\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, i_data in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=n_batch):\n",
    "        pred_mat.append(model.predict(sess, u_data, i_data))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "def recall_cond_fn(label):\n",
    "    \"\"\"recall條件限制為, user在test data正向評價電影至少10部\"\"\"\n",
    "    return sum(label >= 4) >= 10\n",
    "    \n",
    "print( recall_at_k(teRatingMat, pred_mat, condition_fn=recall_cond_fn) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## MF + DNN with Cross Entropy Loss Function (Sigmoid Cross Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在Rating只有1(正向) or 0(負向)時該如何處理?\n",
    "1. 繼承ModelMfDNN, 改動loss function, 因為使用sigmoid, rating欄位需要變成0 or 1\n",
    "2. 每一個epoch觀察cross entropy的loss之外, 也觀察RMSE loss的變化(計算RMSE維持原rating)\n",
    "3. 許多狀況很難拿到user explicit feedback, 像是movie rating, 因為沒法強迫user去rating所有movies, \n",
    "   但是應該都能夠蒐集到implicit feedback\n",
    "   ex: youtube影片觀看時間長短, 點擊率, 通常implicit feedback只能歸納為到 [0, 1] 之間, 0代表negative, 1代表positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMfDNNCrossEntropy(ModelMfDNN):\n",
    "    def __init__(self,\n",
    "                 n_items,\n",
    "                 n_genres,\n",
    "                 dim=32,\n",
    "                 learning_rate=0.01,\n",
    "                 modelDir=\"./model/model_mf_with_dnn_xent\"):\n",
    "        \"\"\"初始化 ModelBase Tensorflow Graph\"\"\"\n",
    "        self.n_items = n_items\n",
    "        self.n_genres = n_genres\n",
    "        self.ftr_cols = OrderedDict()\n",
    "\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.isTrain = tf.placeholder(tf.bool, None)\n",
    "                # user data\n",
    "                self.user_id = tf.placeholder(tf.int32, [None])\n",
    "                self.query_movie_ids = tf.placeholder(tf.int32, [None, None])\n",
    "                self.query_movie_ids_len = tf.placeholder(tf.int32, [None])\n",
    "                # item data\n",
    "                self.genres = tf.placeholder(tf.int32, [None, None])\n",
    "                self.genres_len = tf.placeholder(tf.int32, [None])\n",
    "                self.avg_rating = tf.placeholder(tf.float32, [None])\n",
    "                self.year = tf.placeholder(tf.float32, [None])\n",
    "                self.candidate_movie_id = tf.placeholder(tf.int32, [None])\n",
    "                # labels\n",
    "                self.rating = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "            init_fn = tf.glorot_normal_initializer()\n",
    "            emb_init_fn = tf.glorot_uniform_initializer()\n",
    "            self.b_global = tf.Variable(emb_init_fn(shape=[]), name=\"b_global\")\n",
    "            with tf.variable_scope(\"embedding\"):\n",
    "                self.w_query_movie_ids = tf.Variable(emb_init_fn(shape=[self.n_items, dim]), name=\"w_query_movie_ids\")\n",
    "                self.b_query_movie_ids = tf.Variable(emb_init_fn(shape=[dim]), name=\"b_query_movie_ids\")\n",
    "                self.w_candidate_movie_id = tf.Variable(init_fn(shape=[self.n_items, dim]), name=\"w_candidate_movie_id\")\n",
    "                self.b_candidate_movie_id = tf.Variable(init_fn(shape=[dim + 8 + 2]), name=\"b_candidate_movie_id\")\n",
    "                self.w_genres = tf.Variable(emb_init_fn(shape=[self.n_genres, 8]), name=\"w_genres\")\n",
    "\n",
    "                # query_movie embedding\n",
    "                self.query_emb = tf.nn.embedding_lookup(self.w_query_movie_ids, self.query_movie_ids)\n",
    "                query_movie_mask = tf.expand_dims(tf.nn.l2_normalize(tf.to_float(tf.sequence_mask(self.query_movie_ids_len)), 1), -1)\n",
    "                self.query_emb = tf.reduce_sum(self.query_emb * query_movie_mask, 1)\n",
    "                self.query_bias = tf.matmul(self.query_emb, self.b_query_movie_ids[:, tf.newaxis])\n",
    "\n",
    "                # candidate_movie embedding\n",
    "                self.candidate_emb = tf.nn.embedding_lookup(self.w_candidate_movie_id, self.candidate_movie_id)\n",
    "\n",
    "                # genres embedding\n",
    "                self.genres_emb = tf.nn.embedding_lookup(self.w_genres, tf.to_int32(self.genres))\n",
    "                genres_mask = tf.expand_dims( tf.nn.l2_normalize(tf.to_float(tf.sequence_mask( tf.reshape(self.genres_len, [-1])) ), 1), -1)\n",
    "                self.genres_emb = tf.reduce_sum(self.genres_emb * genres_mask, 1)\n",
    "\n",
    "            # encode [item embedding + item metadata]\n",
    "            with tf.variable_scope(\"dnn\"):\n",
    "                self.item_repr = tf.concat([self.candidate_emb, self.genres_emb, self.avg_rating[:, tf.newaxis], self.year[:, tf.newaxis]], 1)\n",
    "                self.candidate_bias = tf.matmul(self.item_repr, self.b_candidate_movie_id[:, tf.newaxis])\n",
    "                \n",
    "                # Do: 若有overfitting後可嘗試dropout\n",
    "                # Do: 目前兩層DNN, 可以試著增加或減少hidden layer, 但是最後要跟query_emb內積, 最後一層維度必須 = dim\n",
    "                # Example:\n",
    "                    # self.item_repr = tf.layers.dense(self.item_repr, 64, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                    # self.item_repr = tf.layers.dense(self.item_repr, 32, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                    # self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu) \n",
    "                # dp_scale = 0.5\n",
    "                # self.item_repr = tf.layers.dropout(self.item_repr, dp_scale, training=self.isTrain)\n",
    "                self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                # self.item_repr = tf.layers.dropout(self.item_repr, dp_scale, training=self.isTrain)\n",
    "                self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                # self.item_repr = tf.layers.dropout(self.item_repr, dp_scale, training=self.isTrain)\n",
    "\n",
    "            with tf.variable_scope(\"computation\"):\n",
    "                infer = tf.reduce_sum(self.query_emb * self.item_repr, 1, keep_dims=True)\n",
    "                infer = tf.add(infer, self.b_global)\n",
    "                infer = tf.add(infer, self.query_bias)\n",
    "                self.infer = tf.add(infer, self.candidate_bias, name=\"infer\")\n",
    "\n",
    "                # one query for all items\n",
    "                self.pred = tf.matmul(self.query_emb, tf.transpose(self.item_repr)) + \\\n",
    "                            tf.reshape(self.candidate_bias, (1, -1)) + self.query_bias + self.b_global\n",
    "                self.pred = tf.nn.sigmoid(self.pred)\n",
    "                pass\n",
    "\n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                # if rating >= 4 then 1 else 0\n",
    "                self.alter_rating = tf.to_float(self.rating >= 4)\n",
    "                self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.alter_rating[:, np.newaxis], logits=self.infer))\n",
    "\n",
    "                # for eval\n",
    "                self.rmse_loss = tf.sqrt(tf.losses.mean_squared_error(labels=self.alter_rating[:, tf.newaxis], predictions=self.infer))\n",
    "                self.mae_loss = tf.reduce_mean(tf.abs(self.infer - self.alter_rating[:, tf.newaxis]))\n",
    "                pass\n",
    "\n",
    "            with tf.variable_scope(\"train\"):\n",
    "                # try: 選擇不同的optimizer\n",
    "                self.train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "                # self.train_op = tf.train.AdagradOptimizer(learning_rate).minimize(self.loss)\n",
    "                # self.train_op = tf.train.RMSPropOptimizer(learning_rate).minimize(self.loss)\n",
    "                # self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "                pass\n",
    "\n",
    "            self.saver = tf.train.Saver(tf.global_variables())\n",
    "            self.graph = graph\n",
    "            self.modelDir = modelDir\n",
    "            \n",
    "    def fit(self, sess, trainGen, testGen, reset=False, nEpoch=50):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if reset:\n",
    "            print(\"reset model: clean model dir: {} ...\".format(self.modelDir))\n",
    "            self.resetModel(self.modelDir)\n",
    "        # try: 試著重上次儲存的model再次training\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "\n",
    "        start = time.time()\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (\"Epoch\", \"XEntropy Error\", \"Val XENT Error\", \"Val RMSE Error\", \"Elapsed Time\"))\n",
    "        minLoss = 1e7\n",
    "        for ep in range(1, nEpoch + 1):\n",
    "            tr_loss, tr_total =  0, 0\n",
    "            for i, data in enumerate(trainGen(), 1):\n",
    "                loss, _ = sess.run([self.loss, self.train_op], feed_dict=self.feed_dict(data, mode=\"train\"))\n",
    "                batch_len = len(data[\"query_movie_ids\"])\n",
    "                tr_loss += loss * batch_len\n",
    "                tr_total += batch_len\n",
    "                print(\"\\rtrain loss: {:.3f}\".format(loss), end=\"\")\n",
    "                \n",
    "            if testGen is not None:\n",
    "                te_rmse_loss, te_xent_loss  = self.epochLoss(sess, testGen)\n",
    "\n",
    "            tpl = \"\\r%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\"\n",
    "            if minLoss > te_xent_loss:\n",
    "                tpl += \", saving ...\"\n",
    "                self.saver.save(sess, os.path.join(self.modelDir, 'model'), global_step=ep)\n",
    "                minLoss = te_xent_loss\n",
    "\n",
    "            end = time.time()\n",
    "            print(tpl % (ep, tr_loss / tr_total, te_xent_loss, te_rmse_loss, end - start))\n",
    "            start = end\n",
    "        return self\n",
    "\n",
    "    def epochLoss(self, sess, dataGen):\n",
    "        tot_xent_loss, tot_rmse_loss, tot_cnt = 0, 0, 0\n",
    "        for data in dataGen():\n",
    "            xent_loss, rmse_loss = sess.run([self.loss, self.rmse_loss], feed_dict=self.feed_dict(data, mode=\"eval\"))\n",
    "            batch_len = len(data[\"query_movie_ids\"])\n",
    "            tot_rmse_loss += rmse_loss ** 2 * batch_len\n",
    "            tot_xent_loss += xent_loss * batch_len\n",
    "            tot_cnt += batch_len\n",
    "        return np.sqrt(tot_rmse_loss / tot_cnt), tot_xent_loss / tot_cnt\n",
    "    \n",
    "    \n",
    "learning_rate = 0.1\n",
    "dim = 16\n",
    "modelDir = \"./model/model_mf_with_dnn_xent\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model_xent = ModelMfDNNCrossEntropy(\n",
    "                n_items=nMovies,\n",
    "                n_genres=n_genres,\n",
    "                dim=dim,\n",
    "                learning_rate=learning_rate,\n",
    "                modelDir=modelDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_batch = 128\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    model_xent.fit(sess, dataFn(trProcessed, n_batch=n_batch, shuffle=True), dataFn(teProcessed, n_batch=n_batch), nEpoch=10, reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC_CURVE (Receiver operating characteristic), AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def drawRocCurve(y, predProba):\n",
    "    fprRf, tprRf, _ = roc_curve(y, predProba, pos_label=1)\n",
    "    aucScr = auc(fprRf, tprRf)\n",
    "    print(\"auc:\", aucScr)\n",
    "    f, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.plot(fprRf, tprRf, label='ROC CURVE')\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.set_title('AOC: Area Under Curve (score: {:.4f})'.format(aucScr))\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "coord = teRatingMat.nonzero()\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    predMat = []\n",
    "    for u_data, items in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=128):\n",
    "        predMat.append(model_xent.predict(sess, u_data, items))\n",
    "    predMat = np.vstack(predMat)\n",
    "# regard rating >= 4 as user like this movie\n",
    "drawRocCurve((teRatingMat[coord] >= 4).astype(int), predMat[coord])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 觀察單一user與預測分布圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# user id from 0 ~ 670\n",
    "uid = 26\n",
    "u_queries, movies_meta = list(user_item_data(trProcessed, [uid], movie_trans))[0]\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    pred = model_xent.predict(sess, u_queries, movies_meta)\n",
    "print(\"shape: \", pred.shape, minmax_scale(pred.T).T)\n",
    "\n",
    "nnzCoord = teRatingMat[uid].nonzero()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"pred distribution\")\n",
    "pd.Series(pred.ravel()[nnzCoord]).hist(bins=30, ax=ax[0])\n",
    "ax[1].set_title(\"real distribution\")\n",
    "pd.Series(map(lambda e: e, teRatingMat[uid][nnzCoord])).hist(bins=30, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User導向評估(Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 可給定user id細看每個user的rating與model預測效果\n",
    "# valid user id from 0 ~ 670\n",
    "uid = 26\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    u_queries, movies_meta = list(user_item_data(trProcessed, [uid], movie_trans))[0]\n",
    "    recomm = model_xent.predict(sess, u_queries, movies_meta).ravel()\n",
    "recommDf = pd.DataFrame(data={\n",
    "              \"userId\": uid,\n",
    "              \"movieId\": range(len(recomm)), \n",
    "              \"title\": midMap[np.arange(len(recomm))].values, \n",
    "              \"rating\": teRatingMat[uid, range(len(recomm))],\n",
    "              \"predRating\": recomm},\n",
    "             columns=(\"userId\", \"movieId\", \"title\", \"rating\", \"predRating\"))\n",
    "# ascending 可以調整True or False觀察結果\n",
    "recommDf.query(\"rating != 0\").sort_values(\"rating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model導向評估(Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommDf.query(\"rating != 0\").sort_values(\"predRating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision At 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def precision_at_k(truth, pred_mat, condition_fn=None, k=10, label_thres=4):\n",
    "    hits, total = 0, 0\n",
    "    for label, pr in zip(truth, pred_mat):\n",
    "        if not condition_fn(label): continue\n",
    "\n",
    "        top_k_ind = (pr * (label != 0)).argsort()[::-1][:k]\n",
    "        hits += sum(label[top_k_ind] >= label_thres)\n",
    "        total += k\n",
    "    return hits / total\n",
    "\n",
    "n_batch = 128\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, i_data in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=n_batch):\n",
    "        pred_mat.append(model_xent.predict(sess, u_data, i_data))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "print( \"strict condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, strict_condition, k=10) )\n",
    "print( \"norm condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, norm_condition, k=10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, items in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=128):\n",
    "        pred_mat.append(model_xent.predict(sess, u_data, items))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "strict_ndcg = all_user_ndcg(teRatingMat, pred_mat, strict_condition, label_thres=4, k=10)\n",
    "norm_ndcg = all_user_ndcg(teRatingMat, pred_mat, norm_condition, label_thres=4, k=10)\n",
    "print(\"strict condition ndcg at 10: \", strict_ndcg)\n",
    "print(\"norm condition ndcg at 10: \", norm_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## 取出movies embedding, 使用cosine similarity列出最相似的電影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies[movies.title.str.contains(\"Toy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "def most_like(model, seed_movie, k=10):\n",
    "    \"\"\"給定某一部電影, 使用model裡movies embedding找尋cosine相似度高的其他電影!\"\"\"\n",
    "    with tf.Session(graph=model.graph) as sess:\n",
    "        model.ckpt(sess, model.modelDir)\n",
    "        user_queries, items = list(user_item_data(trProcessed, [0], movie_trans))[0]\n",
    "        movie_emb = sess.run(model.item_repr, feed_dict={\n",
    "            model.isTrain: False,\n",
    "            model.genres: items[\"genres\"],\n",
    "            model.genres_len: items[\"genres_len\"],\n",
    "            model.avg_rating: items[\"avg_rating\"],\n",
    "            model.year: items[\"year\"],\n",
    "            model.candidate_movie_id: items[\"candidate_movie_id\"]\n",
    "        })\n",
    "        \n",
    "    most_like = cosine_similarity(movie_emb[seed_movie][np.newaxis, :], movie_emb).ravel().argsort()[::-1][:k]\n",
    "    return movies.iloc[most_like]\n",
    "\n",
    "# mse訓練出來的model\n",
    "most_like(model, 0, k=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cross entropy訓練出來的model\n",
    "most_like(model_xent, 0, k=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Observations\n",
    "1. 可觀察到使用movie embedding以cosine相似度找相似的movie已經有效果, 但是我們用的並不是自己組合出來的movie features, 而是embedding, 代表在training當中已經讓embedding是有意義的代表movie.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
