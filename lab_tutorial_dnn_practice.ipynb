{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, matplotlib.pyplot as plt, collections\n",
    "\n",
    "def reload(mName):\n",
    "    import importlib\n",
    "    if mName in sys.modules:\n",
    "        del sys.modules[mName]\n",
    "    return importlib.import_module(mName)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "utils = reload('utils.utils')\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "np.random.seed(88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target: 預測會出現哪一隻神奇寶貝(5種)\n",
    "1. Features dimensions = 200, 時間 天氣 位置 環境等等\n",
    "2. Label dimensions = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' Read input files '''\n",
    "my_data = np.genfromtxt('./data/pkgo_city66_class5_v1.csv', delimiter=',',skip_header=1)\n",
    "\n",
    "''' The first column to the 199th column is used as input features '''\n",
    "X_train = my_data[:,0:200]\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "''' The 200-th column is the answer '''\n",
    "y_train = my_data[:,200]\n",
    "y_train = y_train.astype('int')\n",
    "\n",
    "''' Convert to one-hot encoding '''\n",
    "Y_train = tf.keras.utils.to_categorical(y_train,5)\n",
    "\n",
    "''' Shuffle training data '''\n",
    "from sklearn.utils import shuffle\n",
    "X_train, Y_train = shuffle(X_train, Y_train, random_state=100)\n",
    "\n",
    "''' Split valid data from train data '''\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.1)\n",
    "print(\"train data shape: {}, valid data shape: {}\".format(X_train.shape, X_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator Function: Fetch Data Per Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_fn(X_train, Y_train, n_batch, shuffle=False):\n",
    "    \"\"\"train data iterator\"\"\"\n",
    "    def fn():\n",
    "        indices = utils.get_minibatches_idx(len(X_train), batch_size=n_batch, shuffle=shuffle)\n",
    "        for ind in indices:\n",
    "            yield X_train[ind], Y_train[ind]\n",
    "    return fn\n",
    "\n",
    "''' preview training data '''\n",
    "for data, label in data_fn(X_train, Y_train, n_batch=6, shuffle=False)():\n",
    "    print(\"data (shape: {}):\\n{}\".format(data.shape, data))\n",
    "    print()\n",
    "    print(\"label (shape: {}):\\n{}\".format(label.shape, label))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model\n",
    "1. Extend this base model, training與eval的動作就不用重寫, 只需要注重在graph與hyperparameter的修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    def fit(self, train_fn, valid_fn, n_epoch, lr, callback=None):\n",
    "        \"\"\"Training\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "            train_fn: train data batch generator function\n",
    "            valid_fn: valid data batch generator function\n",
    "            n_epoch: number of times through all train data\n",
    "            lr: learning rate\n",
    "            callback: call on every epoch end\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "            self\n",
    "        \"\"\"\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # assign learning rate\n",
    "            sess.run(tf.assign(self.lr, lr))\n",
    "            \n",
    "            self.hist = {\"tr_loss\": [], \"tr_acc\": [], \"vl_loss\": [], \"vl_acc\": []}\n",
    "            for ep in range(1, n_epoch + 1):\n",
    "                try:\n",
    "                    tr_loss, tr_acc, tr_len = 0, 0, 0\n",
    "                    for i, (tr_x, tr_y) in enumerate(train_fn(), 1):\n",
    "                        loss, acc, _ = sess.run([self.loss, self.acc, self.train_op], feed_dict=self.feed_dict(tr_x, tr_y, mode=\"train\"))\n",
    "                        l = len(tr_x)\n",
    "                        tr_loss += loss * l\n",
    "                        tr_acc += acc * l\n",
    "                        tr_len += l\n",
    "\n",
    "                    vl_loss, vl_acc = self.evaluate(sess, valid_fn)\n",
    "                    tr_loss, tr_acc = tr_loss / tr_len, tr_acc / tr_len\n",
    "                    self.hist[\"tr_loss\"].append(tr_loss)\n",
    "                    self.hist[\"tr_acc\"].append(tr_acc)\n",
    "                    self.hist[\"vl_loss\"].append(vl_loss)\n",
    "                    self.hist[\"vl_acc\"].append(vl_acc)\n",
    "                    if callback is not None:\n",
    "                        callback(self)\n",
    "                    print(\"\\r{}/{}.  train loss: {:.3f}, valid loss: {:.3f}, train acc: {:.3f}, valid acc: {:.3f}\".\\\n",
    "                          format(ep, n_epoch, tr_loss, vl_loss, tr_acc, vl_acc), end=\"\")\n",
    "                except StopIteration as si: \n",
    "                    print(si)\n",
    "                    break\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, sess, valid_fn):\n",
    "        \"\"\"算出valid data的所有資料loss, accuracy\"\"\"\n",
    "        vl_loss, vl_acc, total = 0, 0, 0\n",
    "        for vl_x, vl_y in valid_fn():\n",
    "            loss, acc = sess.run([self.loss, self.acc], feed_dict=self.feed_dict(vl_x, vl_y, mode=\"eval\"))\n",
    "            l = len(vl_x)\n",
    "            vl_loss += loss * l\n",
    "            vl_acc += acc * l\n",
    "            total += l\n",
    "        return vl_loss / total, vl_acc / total\n",
    "    \n",
    "    def feed_dict(self, x, y, mode=\"train\"):\n",
    "        \"\"\"建立餵給tensorflow的dictionary\"\"\"\n",
    "        assert mode in (\"train\", \"eval\", \"infer\"), \"mode not in (train, eval, infer)\"\n",
    "        ret = {self.inputs: x, self.labels: y}\n",
    "        if mode == \"train\":\n",
    "            ret[self.is_train] = True\n",
    "        else:\n",
    "            ret[self.is_train] = False\n",
    "        return ret\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"Loss and Accuracy plot\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(self.hist[\"tr_loss\"], label='Train', marker=\"o\", markersize=5)\n",
    "        plt.plot(self.hist[\"vl_loss\"], label='Valid', marker=\"o\", markersize=5)\n",
    "        plt.title('Loss')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.subplot(122)\n",
    "        plt.plot(self.hist[\"tr_acc\"], label='Train', marker=\"o\", markersize=5)\n",
    "        plt.plot(self.hist[\"vl_acc\"], label='Valid', marker=\"o\", markersize=5)\n",
    "        plt.title('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification With MSE\n",
    "```\n",
    "+ learning_rate: 0.01 \n",
    "+ loss function: mean square error\n",
    "+ optimizer: 基本款 => GradientDescentOptimizer\n",
    "+ activation function: sigmoid\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 調整learning rate\n",
    "learning_rate = 0.01\n",
    "# 調整每個batch資料數量\n",
    "n_batch = 16\n",
    "# 調整epoch數量, 一個epoch代表跑完一次train data\n",
    "n_epoch = 30\n",
    "\n",
    "class ModelMSE(BaseModel):\n",
    "    def __init__(self):\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.inputs = tf.placeholder(tf.float32, [None, 200], name=\"inputs\")\n",
    "                self.labels = tf.placeholder(tf.float32, [None, 5], name=\"labels\")\n",
    "                self.is_train = tf.placeholder(tf.bool, None, name=\"is_train\")\n",
    "                # learning rate init value = 0.1\n",
    "                self.lr = tf.Variable(0.1, trainable=False)\n",
    "\n",
    "            init_fn = tf.glorot_normal_initializer()\n",
    "            with tf.variable_scope(\"dnn\"):\n",
    "                nets = tf.layers.dense(self.inputs, 128, kernel_initializer=init_fn, activation=tf.nn.sigmoid, name=\"dnn1\")\n",
    "                nets = tf.layers.dense(nets, 256, kernel_initializer=init_fn, activation=tf.nn.sigmoid, name=\"dnn2\")\n",
    "                nets = tf.layers.dense(nets, 5, kernel_initializer=init_fn, activation=None, name=\"dnn3\")\n",
    "                self.pred = tf.nn.softmax(nets, name=\"pred\")\n",
    "                \n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                # 使用 mean squared error loss function\n",
    "                self.loss = tf.losses.mean_squared_error(labels=self.labels, predictions=self.pred)\n",
    "                \n",
    "            with tf.variable_scope(\"eval\"):\n",
    "                # 計算 accuracy\n",
    "                self.acc = tf.reduce_mean( tf.to_float(tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.labels, 1))) )\n",
    "\n",
    "            with tf.variable_scope(\"train\"):\n",
    "                self.train_op = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss)\n",
    "                \n",
    "        self.graph = graph\n",
    "        \n",
    "print(\"mean square error model!\")\n",
    "tf.reset_default_graph()\n",
    "model_mse = ModelMSE()\n",
    "model_mse.fit(data_fn(X_train, Y_train, n_batch, shuffle=True), data_fn(X_valid, Y_valid, n_batch, shuffle=False), n_epoch, lr=learning_rate)\n",
    "model_mse.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification With Cross Entropy\n",
    "```\n",
    "+ learning_rate: 0.01 \n",
    "+ loss function: softmax cross entropy\n",
    "+ optimizer: 基本款 => GradientDescentOptimizer\n",
    "+ activation function: sigmoid\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 調整learning rate\n",
    "learning_rate = 0.01\n",
    "# 調整每個batch資料數量\n",
    "n_batch = 16 \n",
    "# 調整epoch數量, 一個epoch代表跑完一次train data\n",
    "n_epoch = 30\n",
    "\n",
    "class ModelCE(BaseModel):\n",
    "    def __init__(self):\n",
    "        # super(Model, self).__init__()\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.inputs = tf.placeholder(tf.float32, [None, 200], name=\"inputs\")\n",
    "                self.labels = tf.placeholder(tf.float32, [None, 5], name=\"labels\")\n",
    "                self.is_train = tf.placeholder(tf.bool, None, name=\"is_train\")\n",
    "                # learning rate init value = 0.1\n",
    "                self.lr = tf.Variable(0.1, trainable=False)\n",
    "\n",
    "            init_fn = tf.glorot_normal_initializer()\n",
    "            with tf.variable_scope(\"dnn\"):\n",
    "                nets = tf.layers.dense(self.inputs, 128, kernel_initializer=init_fn, activation=tf.nn.sigmoid, name=\"dnn1\")\n",
    "                nets = tf.layers.dense(nets, 256, kernel_initializer=init_fn, activation=tf.nn.sigmoid, name=\"dnn2\")\n",
    "                nets = tf.layers.dense(nets, 5, kernel_initializer=init_fn, activation=None, name=\"dnn3\")\n",
    "\n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                # Do: loss function 改用 cross entropy\n",
    "                \n",
    "                \n",
    "            with tf.variable_scope(\"eval\"):\n",
    "                self.pred = tf.nn.softmax(nets, name=\"pred\")\n",
    "                self.acc = tf.reduce_mean( tf.to_float(tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.labels, 1))) )\n",
    "\n",
    "            with tf.variable_scope(\"train\"):\n",
    "                self.train_op = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss)\n",
    "                \n",
    "        self.graph = graph\n",
    "        \n",
    "print(\"cross entropy model!\")\n",
    "tf.reset_default_graph()\n",
    "model_ce = ModelCE()\n",
    "model_ce.fit( data_fn(X_train, Y_train, n_batch, shuffle=True), data_fn(X_valid, Y_valid, n_batch, shuffle=False), n_epoch, lr=learning_rate)\n",
    "model_ce.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Learning Rate!\n",
    "```\n",
    "+ learning_rate: 0.1, 0.01, 0.001\n",
    "+ loss function: softmax cross entropy\n",
    "+ optimizer: 基本款 => GradientDescentOptimizer\n",
    "+ activation function: sigmoid\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 調整learning rate\n",
    "learning_rate = [0.1, 0.01, 0.001]\n",
    "# 調整每個batch資料數量\n",
    "n_batch = 16 \n",
    "# 調整epoch數量, 一個epoch代表跑完一次train data\n",
    "n_epoch = 30\n",
    "        \n",
    "print(\"Try learning rate!\")\n",
    "hist = collections.OrderedDict()\n",
    "tf.reset_default_graph()\n",
    "model_ce = ModelCE()\n",
    "for lr in learning_rate:\n",
    "    model_ce.fit( data_fn(X_train, Y_train, n_batch, shuffle=True), data_fn(X_valid, Y_valid, n_batch, shuffle=False), n_epoch, lr)\n",
    "    hist[str(lr)] = model_ce.hist\n",
    "    print(\"  learning rate: {} done !\".format(lr))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(hist[\"0.1\"][\"tr_loss\"], label='lr=0.1', marker=\"o\", markersize=5)\n",
    "plt.plot(hist[\"0.01\"][\"tr_loss\"], label='lr=0.01', marker=\"o\", markersize=5)\n",
    "plt.plot(hist[\"0.001\"][\"tr_loss\"], label='lr=0.001', c=\"g\", marker=\"o\", markersize=5)\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(122)\n",
    "plt.plot(hist[\"0.1\"][\"tr_acc\"], label='lr=0.1', marker=\"o\", markersize=5)\n",
    "plt.plot(hist[\"0.01\"][\"tr_acc\"], label='lr=0.01', marker=\"o\", markersize=5)\n",
    "plt.plot(hist[\"0.001\"][\"tr_acc\"], label='lr=0.001', c=\"g\", marker=\"o\", markersize=5)\n",
    "plt.title('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Activation Function\n",
    "```\n",
    "+ learning_rate: 0.01\n",
    "+ loss function: softmax cross entropy\n",
    "+ optimizer: 基本款 => GradientDescentOptimizer\n",
    "+ activation function: sigmoid or softplus\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 調整learning rate\n",
    "learning_rate = 0.01\n",
    "# 調整每個batch資料數量\n",
    "n_batch = 16 \n",
    "# 調整epoch數量, 一個epoch代表跑完一次train data\n",
    "n_epoch = 30\n",
    "\n",
    "class ModelSigmoid(BaseModel):\n",
    "    def __init__(self):\n",
    "        # super(Model, self).__init__()\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.inputs = tf.placeholder(tf.float32, [None, 200], name=\"inputs\")\n",
    "                self.labels = tf.placeholder(tf.float32, [None, 5], name=\"labels\")\n",
    "                self.is_train = tf.placeholder(tf.bool, None, name=\"is_train\")\n",
    "                # learning rate init value = 0.1\n",
    "                self.lr = tf.Variable(0.1, trainable=False)\n",
    "\n",
    "            init_fn = tf.glorot_normal_initializer()\n",
    "            with tf.variable_scope(\"dnn\"):\n",
    "                nets = tf.layers.dense(self.inputs, 128, kernel_initializer=init_fn, activation=tf.nn.sigmoid, name=\"dnn1\")\n",
    "                nets = tf.layers.dense(nets, 256, kernel_initializer=init_fn, activation=tf.nn.sigmoid, name=\"dnn2\")\n",
    "                nets = tf.layers.dense(nets, 5, kernel_initializer=init_fn, activation=None, name=\"dnn3\")\n",
    "\n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=nets))\n",
    "                \n",
    "            with tf.variable_scope(\"eval\"):\n",
    "                self.pred = tf.nn.softmax(nets, name=\"pred\")\n",
    "                self.acc = tf.reduce_mean( tf.to_float(tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.labels, 1))) )\n",
    "\n",
    "            with tf.variable_scope(\"train\"):\n",
    "                self.train_op = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss)\n",
    "                \n",
    "        self.graph = graph\n",
    "\n",
    "class ModelSoftPlus(BaseModel):\n",
    "    def __init__(self):\n",
    "        # super(Model, self).__init__()\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.inputs = tf.placeholder(tf.float32, [None, 200], name=\"inputs\")\n",
    "                self.labels = tf.placeholder(tf.float32, [None, 5], name=\"labels\")\n",
    "                self.is_train = tf.placeholder(tf.bool, None, name=\"is_train\")\n",
    "                # learning rate init value = 0.1\n",
    "                self.lr = tf.Variable(0.1, trainable=False)\n",
    "\n",
    "            init_fn = tf.glorot_normal_initializer()\n",
    "            with tf.variable_scope(\"dnn\"):\n",
    "                # Do: activation改成tf.nn.softplus, 請修改問號!\n",
    "                nets = tf.layers.dense(self.inputs, 128, kernel_initializer=init_fn, activation=?, name=\"dnn1\")\n",
    "                nets = tf.layers.dense(nets, 256, kernel_initializer=init_fn, activation=?, name=\"dnn2\")\n",
    "                nets = tf.layers.dense(nets, 5, kernel_initializer=init_fn, activation=None, name=\"dnn3\")\n",
    "\n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=nets))\n",
    "                \n",
    "            with tf.variable_scope(\"eval\"):\n",
    "                self.pred = tf.nn.softmax(nets, name=\"pred\")\n",
    "                self.acc = tf.reduce_mean( tf.to_float(tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.labels, 1))) )\n",
    "\n",
    "            with tf.variable_scope(\"train\"):\n",
    "                self.train_op = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss)\n",
    "                \n",
    "        self.graph = graph\n",
    "        \n",
    "print(\"model with sigmoid activation function!\")\n",
    "tf.reset_default_graph()\n",
    "model_sm = ModelSigmoid()\n",
    "model_sm.fit( data_fn(X_train, Y_train, n_batch, shuffle=True), data_fn(X_valid, Y_valid, n_batch, shuffle=False), n_epoch, lr=learning_rate)\n",
    "\n",
    "print(\"\\n\\nmodel with softplus activation function!\")\n",
    "tf.reset_default_graph()\n",
    "model_sp = ModelSoftPlus()\n",
    "model_sp.fit( data_fn(X_train, Y_train, n_batch, shuffle=True), data_fn(X_valid, Y_valid, n_batch, shuffle=False), n_epoch, lr=learning_rate)\n",
    "\n",
    "hist = {\"sigmoid\": model_sm.hist, \"softplus\": model_sp.hist}\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(hist[\"softplus\"][\"tr_loss\"], label='Softplus', marker=\"o\", markersize=5)\n",
    "plt.plot(hist[\"sigmoid\"][\"tr_loss\"], label='Sigmoid', marker=\"o\", markersize=5)\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(122)\n",
    "plt.plot(hist[\"softplus\"][\"tr_acc\"], label='Softplus', marker=\"o\", markersize=5)\n",
    "plt.plot(hist[\"sigmoid\"][\"tr_acc\"], label='Sigmoid', marker=\"o\", markersize=5)\n",
    "plt.title('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Different Optimizer\n",
    "```\n",
    "+ learning_rate: 0.01\n",
    "+ loss function: softmax cross entropy\n",
    "+ optimizer: GradientDescentOptimizer or AdamOptimizer\n",
    "+ activation function: softplus\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 調整learning rate\n",
    "learning_rate = 0.01\n",
    "# 調整每個batch資料數量\n",
    "n_batch = 16 \n",
    "# 調整epoch數量, 一個epoch代表跑完一次train data\n",
    "n_epoch = 30\n",
    "\n",
    "class ModelAdam(BaseModel):\n",
    "    def __init__(self):\n",
    "        # super(Model, self).__init__()\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.inputs = tf.placeholder(tf.float32, [None, 200], name=\"inputs\")\n",
    "                self.labels = tf.placeholder(tf.float32, [None, 5], name=\"labels\")\n",
    "                self.is_train = tf.placeholder(tf.bool, None, name=\"is_train\")\n",
    "                # learning rate init value = 0.1\n",
    "                self.lr = tf.Variable(0.1, trainable=False)\n",
    "\n",
    "            init_fn = tf.glorot_normal_initializer()\n",
    "            with tf.variable_scope(\"dnn\"):\n",
    "                nets = tf.layers.dense(self.inputs, 128, kernel_initializer=init_fn, activation=tf.nn.softplus, name=\"dnn1\")\n",
    "                nets = tf.layers.dense(nets, 256, kernel_initializer=init_fn, activation=tf.nn.softplus, name=\"dnn2\")\n",
    "                nets = tf.layers.dense(nets, 5, kernel_initializer=init_fn, activation=None, name=\"dnn3\")\n",
    "\n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=nets))\n",
    "                \n",
    "            with tf.variable_scope(\"eval\"):\n",
    "                self.pred = tf.nn.softmax(nets, name=\"pred\")\n",
    "                self.acc = tf.reduce_mean( tf.to_float(tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.labels, 1))) )\n",
    "\n",
    "            with tf.variable_scope(\"train\"):\n",
    "                # Do: optimizer改成AdamOptimizer, 修改問號部分\n",
    "                self.train_op = ?\n",
    "        self.graph = graph\n",
    "        \n",
    "print(\"model with Adam optimizer!\")\n",
    "tf.reset_default_graph()\n",
    "model_adam = ModelAdam()\n",
    "model_adam.fit( data_fn(X_train, Y_train, n_batch, shuffle=True), data_fn(X_valid, Y_valid, n_batch, shuffle=False), n_epoch, lr=learning_rate)        \n",
    "        \n",
    "print(\"\\n\\nmodel with SGD optimizer!\")\n",
    "tf.reset_default_graph()\n",
    "model_sp = ModelSoftPlus()\n",
    "model_sp.fit( data_fn(X_train, Y_train, n_batch, shuffle=True), data_fn(X_valid, Y_valid, n_batch, shuffle=False), n_epoch, lr=learning_rate)\n",
    "\n",
    "hist = {\"sgd\": model_sp.hist, \"adam\": model_adam.hist}\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(hist[\"adam\"][\"tr_loss\"], label='Adam', marker=\"o\", markersize=5)\n",
    "plt.plot(hist[\"sgd\"][\"tr_loss\"], label='SGD', marker=\"o\", markersize=5)\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(122)\n",
    "plt.plot(hist[\"adam\"][\"tr_acc\"], label='Adam', marker=\"o\", markersize=5)\n",
    "plt.plot(hist[\"sgd\"][\"tr_acc\"], label='SGD', marker=\"o\", markersize=5)\n",
    "plt.title('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Check\n",
    "```\n",
    "+ learning_rate: 0.001\n",
    "+ loss function: softmax cross entropy\n",
    "+ optimizer: AdamOptimizer\n",
    "+ activation function: softplus\n",
    "+ number of train epoch: 30 to 50 => 故意把training次數增加, 觀察是否有overfitting狀況發生\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 調整learning rate\n",
    "learning_rate = 0.001\n",
    "# 調整每個batch資料數量\n",
    "n_batch = 16 \n",
    "# 調整epoch數量為50\n",
    "n_epoch = 50\n",
    "\n",
    "print(\"model with Adam optimizer and train 50 epochs !\")\n",
    "tf.reset_default_graph()\n",
    "model_adam = ModelAdam()\n",
    "model_adam.fit( data_fn(X_train, Y_train, n_batch, shuffle=True), data_fn(X_valid, Y_valid, n_batch, shuffle=False), n_epoch, lr=learning_rate)\n",
    "model_adam.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "1. 觀察Loss and Accuracy的曲線變化, Training部份越來越完美, 可是Valid部分已經開始崩壞, 這正是Overfitting的象徵\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Overfitting: EarlyStopping\n",
    "### 檢查Valid Loss變化, 若發現Loss持續增加超過一定Threshold, 則停止Training\n",
    "```\n",
    "+ learning_rate: 0.001\n",
    "+ loss function: softmax cross entropy\n",
    "+ optimizer: AdamOptimizer\n",
    "+ activation function: softplus\n",
    "+ number of train epoch: 50\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 調整learning rate\n",
    "learning_rate = 0.001\n",
    "# 調整每個batch資料數量\n",
    "n_batch = 16 \n",
    "# 調整epoch數量為50\n",
    "n_epoch = 50\n",
    "\n",
    "class EarlyStopping(object):\n",
    "    \"\"\"若valid loss持續增加超過一定次數則raise StopIteration\"\"\"\n",
    "    def __init__(self, thres):\n",
    "        self.patient = 0\n",
    "        self.min_loss = 1e7\n",
    "        self.thres = thres\n",
    "        \n",
    "    def __call__(self, model):\n",
    "        last_vl_loss = model.hist[\"vl_loss\"][-1]\n",
    "        if self.min_loss > last_vl_loss:\n",
    "            self.patient = 0\n",
    "            self.min_loss = last_vl_loss\n",
    "        else:\n",
    "            self.patient += 1\n",
    "        if self.patient >= self.thres:\n",
    "            raise StopIteration(\"\\nEarly Stopping, valid loss keep increasing for {} times ! \"\n",
    "                                \"lowest loss is {:.3f} !\"\n",
    "                                .format(self.patient, self.min_loss))\n",
    "            \n",
    "tf.reset_default_graph()\n",
    "model_adam = ModelAdam()\n",
    "model_adam.fit(data_fn(X_train, Y_train, n_batch, shuffle=True), \n",
    "               data_fn(X_valid, Y_valid, n_batch, shuffle=False), \n",
    "               n_epoch, \n",
    "               lr=learning_rate,\n",
    "               # Do: 修改threshold, 建議值3 ~ 5\n",
    "               callback=EarlyStopping(thres=?))\n",
    "model_adam.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Overfitting: Add Regularizer Term to Loss Function\n",
    "```\n",
    "+ learning_rate: 0.001\n",
    "+ loss function: softmax cross entropy\n",
    "+ optimizer: AdamOptimizer\n",
    "+ activation function: softplus\n",
    "+ number of train epoch: 50\n",
    "\n",
    "+ >>> add l2 regularizer term to each hidden layers with scale 0.01\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 調整learning rate\n",
    "learning_rate = 0.001\n",
    "# 調整每個batch資料數量\n",
    "n_batch = 16 \n",
    "# 調整epoch數量為50\n",
    "n_epoch = 50\n",
    "\n",
    "class ModelAdamRegularized(BaseModel):\n",
    "    def __init__(self):\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.inputs = tf.placeholder(tf.float32, [None, 200], name=\"inputs\")\n",
    "                self.labels = tf.placeholder(tf.float32, [None, 5], name=\"labels\")\n",
    "                self.is_train = tf.placeholder(tf.bool, None, name=\"is_train\")\n",
    "                # learning rate init value = 0.1\n",
    "                self.lr = tf.Variable(0.1, trainable=False)\n",
    "            \n",
    "            init_fn = tf.glorot_normal_initializer()\n",
    "            with tf.variable_scope(\"dnn\"):\n",
    "                # 在各hidden layers加上regularizer term, scale = 0.01\n",
    "                reg_scale = 0.01\n",
    "                # Do: 在kernel_regularizer加上regularizer function, 並帶入reg_scale參數\n",
    "                nets = tf.layers.dense(self.inputs, 128, kernel_initializer=init_fn, activation=tf.nn.softplus,\n",
    "                                       kernel_regularizer=?, name=\"dnn1\")\n",
    "                nets = tf.layers.dense(nets, 256, kernel_initializer=init_fn, activation=tf.nn.softplus, \n",
    "                                       kernel_regularizer=?, name=\"dnn2\")\n",
    "                nets = tf.layers.dense(nets, 5, kernel_initializer=init_fn, activation=None, \n",
    "                                       kernel_regularizer=?, name=\"dnn3\")\n",
    "\n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=nets))\n",
    "                # Do: 在Loss function後加上正規項, unmark下面兩行即可!\n",
    "                # for reg_term in tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES):\n",
    "                #     self.loss += reg_term\n",
    "                \n",
    "            with tf.variable_scope(\"eval\"):\n",
    "                self.pred = tf.nn.softmax(nets, name=\"pred\")\n",
    "                self.acc = tf.reduce_mean( tf.to_float(tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.labels, 1))) )\n",
    "\n",
    "            with tf.variable_scope(\"train\"):\n",
    "                self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        self.graph = graph\n",
    "        \n",
    "tf.reset_default_graph()\n",
    "model_adam_reg = ModelAdamRegularized()\n",
    "model_adam_reg.fit( data_fn(X_train, Y_train, n_batch, shuffle=True), data_fn(X_valid, Y_valid, n_batch, shuffle=False), n_epoch, lr=learning_rate)\n",
    "model_adam_reg.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Overfitting: Dropout\n",
    "```\n",
    "+ learning_rate: 0.001\n",
    "+ loss function: softmax cross entropy\n",
    "+ optimizer: AdamOptimizer\n",
    "+ activation function: softplus\n",
    "\n",
    "+ number of train epoch: 100\n",
    ">>> Dropout會讓training速度變慢, 增加epoch次數讓分數更好!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 調整learning rate\n",
    "learning_rate = 0.001\n",
    "# 調整每個batch資料數量\n",
    "n_batch = 16 \n",
    "# 調整epoch數量為100\n",
    "n_epoch = 100\n",
    "\n",
    "class ModelAdamDropout(BaseModel):\n",
    "    def __init__(self):\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.inputs = tf.placeholder(tf.float32, [None, 200], name=\"inputs\")\n",
    "                self.labels = tf.placeholder(tf.float32, [None, 5], name=\"labels\")\n",
    "                self.is_train = tf.placeholder(tf.bool, None, name=\"is_train\")\n",
    "                # learning rate init value = 0.1\n",
    "                self.lr = tf.Variable(0.1, trainable=False)\n",
    "            \n",
    "            init_fn = tf.glorot_normal_initializer()\n",
    "            with tf.variable_scope(\"dnn\"):\n",
    "                # 在各hidden layers後加上dropout function\n",
    "                # drop rate = 0.5, 每次隨機drop 50%的neurons\n",
    "                drop_rate = 0.5\n",
    "                # Do: 在每一層hidden layer後面加上dropout => tf.layers.dropout(nets, drop_rate, training=self.is_train)\n",
    "                nets = tf.layers.dense(self.inputs, 128, kernel_initializer=init_fn, activation=tf.nn.softplus, name=\"dnn1\")\n",
    "                nets = ?\n",
    "                nets = tf.layers.dense(nets, 256, kernel_initializer=init_fn, activation=tf.nn.softplus, name=\"dnn2\")\n",
    "                nets = ?\n",
    "                nets = tf.layers.dense(nets, 5, kernel_initializer=init_fn, activation=None, name=\"dnn3\")\n",
    "\n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=nets))\n",
    "                \n",
    "            with tf.variable_scope(\"eval\"):\n",
    "                self.pred = tf.nn.softmax(nets, name=\"pred\")\n",
    "                self.acc = tf.reduce_mean( tf.to_float(tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.labels, 1))) )\n",
    "\n",
    "            with tf.variable_scope(\"train\"):\n",
    "                # optimizer改成AdamOptimizer\n",
    "                self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        self.graph = graph\n",
    "        \n",
    "tf.reset_default_graph()\n",
    "model_adam_dp = ModelAdamDropout()\n",
    "model_adam_dp.fit( data_fn(X_train, Y_train, n_batch, shuffle=True), data_fn(X_valid, Y_valid, n_batch, shuffle=False), n_epoch, lr=learning_rate)\n",
    "model_adam_dp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "1. 觀察Loss and Accuracy的曲線變化, Valid Loss曲線在50 epochs之後已經下降變慢, 但並沒有遽增\n",
    "2. Accuracy的部分Valid data也接近75%了\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
